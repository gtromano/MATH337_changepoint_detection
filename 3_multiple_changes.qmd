```{r echo=FALSE, warning=FALSE, message=FALSE}
library(tidyverse)
library(ggpubr)
```

# Multiple changepoints

## Introduction

In real-world data, it is common to encounter situations where more than one change occurs. When applying the CUSUM statistic in such cases, where there are **multiple changes**, the question arises: how does CUSUM behave, and how can we detect these multiple changes effectively?

### Real Example: Genomic Data and Neuroblastoma

To motivate this discussion, we return to the example from week 1: detecting active genomic regions using *ChIP-seq data*. Our goal here is to identify copy number variations (CNVs)---structural changes in the genome where DNA sections are duplicated or deleted. These variations can impact gene expression and are linked to diseases like cancer, including neuroblastoma. The dataset we'll examine consists of logratios of genomic probe intensities, which help us detect changes in the underlying DNA structure.

Statistically our objective is to segment this logratio sequence into regions with different means, corresponding to different genomic states:

```{r echo=FALSE, results=TRUE}
data(neuroblastoma, package="neuroblastoma")

nb.dt <- neuroblastoma[["profiles"]]
one.dt <- nb.dt |> filter(profile.id==4, chromosome==2)

ggplot() +
  scale_y_continuous("logratio (noisy copy number measurement)") +
  geom_point(aes(position / 1e6, logratio),
             data = one.dt) +
  theme_minimal()
```

As seen from the plot, the data is noisy, but there are visible shifts in the logratio values, suggesting multiple changes in the underlying copy number. By the end of this chapter, we will segment this sequence!

### Towards multiple changes

Under this framework, the observed sequence $y_t$ can be modeled as a piecewise constant signal with changes in the mean occurring at each changepoint $\tau_k$. A plausible model for the change-in-mean signal is given by

$$
y_t = \mu_k + \epsilon_t, \quad \text{for} \ \tau_k \leq t < \tau_{k+1}, \ k = 0, 1, \dots, K,
$$

where $\mu_k$ is the mean of the $k$-th segment, and $\epsilon_t \sim \mathcal{N}(0, \sigma^2)$ are independent Gaussian noise terms with mean 0 and (known) variance $\sigma^2$.

As a starting example, we can generate a sequence with 4 segments, with $\tau_1 = 50, \tau_2 = 100, \tau_3 = 150$ and means $\mu_1 = 2, \mu_2 = 0, \mu_3 = -1$ and $\mu_4 = 2$. Running the CUSUM statistic in this scenario with multiple changes, leads to the following $C_\tau^2$ trace:

```{r echo=FALSE}
LR <- function(x) {
  #input is data vector length >=2
  S <- cumsum(x) #calculate cummulate sum of data
  n <- length(x) #number of data points
  tau <- 1:(n - 1) #possible change-point locations to test
  D <- S[tau] / tau - (S[n] - S[tau]) / (n - tau) #difference in means
  LR <- D ^ 2 * tau * (n - tau) / n #LR statistic at locations tau
  #return LR statistic and estimate of tau
  return(list(LR = LR, LR_max = max(LR), tau.hat = which.max(LR)))
}

# broken case for bs
# y <- c(rnorm(100, 2), rnorm(100, 1), rnorm(100, -1), rnorm(100, 1.5))

mu <- c(rep(2, 100), rep(-1, 100), rep(0, 100), rep(2, 100))

set.seed(43)
y <- mu + rnorm(400)
# Calculate CUSUM statistics
cusum_results <- LR(y)

# Create a data frame for the CUSUM trace
cusum_df <- data.frame(
  x = 1:(length(y) - 1),
  y = y[1:(length(y) - 1)],
  mu = mu[1:(length(mu) - 1)],
  CUSUM = cusum_results$LR
)

cs_res_tot <- cusum_df

# Plot 1: Time series
p1 <- ggplot(cusum_df, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = mu), col="red") + 
  theme_minimal()

# Plot 2: CUSUM trace
p2 <- ggplot(cusum_df, aes(x = x, y = CUSUM)) +
  geom_line() +
  theme_minimal()

combined_plot <- ggarrange(p1, p2, align = "hv", nrow = 2)

# Display the plot
print(combined_plot)
```

From this, we notice that our test still has power to detect some of the changes, but the estimate that we get, is initially wrong. $\Delta \mu = |\mu_1 - \mu_2|$. Is power lost when there is more then one change in our test?

Well, to answer this question, we can compare the values of the CUSUM statistic ran on the whole dataset (as above), with the values of the CUSUM, ran on a subset containing only one change:

```{r echo = FALSE}
up <- 200

# Calculate CUSUM statistics
cusum_results <- LR(y[1:up])

# Create a data frame for the CUSUM trace
cusum_df <- data.frame(
  x = 1:(up - 1),
  y = y[1:(up - 1)],
  CUSUM = cusum_results$LR
)

# Plot 1: Time series
p1 <- ggplot(cusum_df, aes(x = x, y = y)) +
  geom_point() +
  theme_minimal()

# Plot 2: CUSUM trace
p2 <- ggplot(cusum_df, aes(x = x, y = CUSUM)) +
  geom_line() +
  geom_line(data = cs_res_tot, aes(x = x, y = CUSUM), col="grey") +
  xlim(1, up) +
  theme_minimal()

combined_plot <- ggarrange(p1, p2, align = "hv", nrow = 2)

# Display the plot
print(combined_plot)
```

We can see that max of the old cusum (the line in grey) is much lower than the one where we isolate the sequence on one single change! So there is an effective loss of power in this scenario in analyzing all changes together, as some changes are masking the effects of others...

This gives us motivation to move towards some methodology that tries to estimate all changes locations jointly, rather then one at a time!

### The cost of a segmentation

Well, so far we only worked with one scheme that tried to split a sequence in a hald

But how can we work in case we have more than one change? Well, we need to introduce the cost of a segment.

If we assume the data is independent and identically distributed within each segment, for segment parameter $\theta$, then this cost can be obtained through: $$
    \mathcal{L}(y_{s+1:t}) = \min_\theta \sum_{i = s + 1}^{t} - \log(f(y_i, \theta))
$$ with $f(y, \theta)$ being the likelihood for data point $y$ if the segment parameter is $\theta$. Now, for example, in the gaussian case this cost is given by:

$$
\mathcal{L}(y_{s:t}) = \frac{1}{2\sigma^2}  \sum_{i = s}^{t} \left ( y_i - \bar{y}_{s:t} \right)^2
$$

The cost for the full segmentation will be given by the sum across all segments:

$$
\sum_{k = 0}^K \mathcal{L}(y_{\tau_k+1:\tau_{k+1}})
$$

Interestingly, the cost of a full segmentation is closely related to the LR test. Consider, a single Gaussian change-in-mean at time $\tau$, splitting the data into two segments: $y_{1:\tau}$ and $y_{\tau+1:n}$. The cost of this segmentation is:

$$
\mathcal{L}(y_{1:\tau}) + \mathcal{L}(y_{\tau+1:n}) = \frac{1}{\sigma^2} \left[\sum_{i=1}^{\tau} (y_i - \bar{y}_{1:\tau})^2 + \sum_{i=\tau+1}^{n} (y_i - \bar{y}_{(\tau+1):n})^2 \right]
$$

Which is essentially the same LR test as we saw last week, without the null component. Specifically, for one change, minimizing the segmentation cost over all possible changepoints locations $\tau$ is equivalent to maximizing the CUSUM statistic.

### The "best" segmentation

We now have a way of evaluating how "good" a segmentation is, so it's only natural to ask the question: what would be the best one?

Well, one way would be to, say, finding the the best set of $\tau = \tau_0, \dots, \tau_{K+1}$ changepoints that minimise the cost:

$$
\min_{\substack{K \in \mathbb{N}\\ \tau_1, \dots, \tau_K}} \sum_{k = 0}^K \mathcal{L}(y_{\tau_k+1:\tau_{k+1}}).
$$ {#eq-segment_cost}

Which one would this be? Say that for instance we range the $K = 1, \dots, n$, and at each step we find the best possible segmentation. Graphically, we would be observing the following:

```{r echo=FALSE, warning=FALSE}
#| fig-height: 2
bs.list <- suppressWarnings(binsegRcpp::binseg_normal(y))
bs.models <- bs.list

gg <- ggplot(cs_res_tot, aes(x = x, y = y)) +
  geom_point()

plotK <- function(k){
  k.segs <- coef(bs.list, as.integer(k))
  gg+
    geom_vline(aes(
      xintercept=start-0.5),
      color="red",
      data=k.segs[-1])+
    geom_segment(aes(
      start-0.5, mean,
      xend=end+0.5, yend=mean),
      size=2,
      alpha=0.5,
      color="red",
      data=k.segs) + 
    theme_minimal() +
    labs(title = paste("Segments:", k, "Seg. Cost: ", round(bs.models$splits$loss[k])), x = "t")
}

plotK(1)
plotK(2)
plotK(3)
plotK(4)

```

Well, arguably we would like to stop at 4, which we know is the real number of segments, but the cost keep going down...

```{r, echo=FALSE}
#| fig-height: 2
plotK(10)
plotK(100)
```

And finally:

```{r echo=FALSE}
#| fig-height: 2
plotK(400)
```

![](source_imgs/fine.png){fig-align="center" width="486"}

Well, it turns out, that according to the minimization above, the optimal segmentation across all would be the one that puts each point into its own segment!

Well, there are different solutions to this problem. The first one we will see, is a divide-and-conquer greedy approach, called Binary Segmentation, and the second one will aim a generating a different optimization to the one below that will find the optimal segmentation *up to a constant* to avoid over-fitting!

## Binary Segmentation

Binary Segmentation (BS) is a procedure from \cite{scott1974cluster} and \cite{sen1975tests}. Binary segmentation works like this:

1.  Start with a test for a change $\tau$ that splits a sequence into two segments and to check if the cost over those two segments, plus a penalty $\beta \in \mathbb{R}$, is smaller then the cost computed on the whole sequence: $$
        \mathcal{L}(y_{1:\tau}) + \mathcal{L}(y_{\tau+1:n}) + \beta < \mathcal{L}(y_{1:n})     
    $$ {#eq-bin_seg_condition}

where the segment cost $\mathcal{L}(\cdot)$, is as in @eq-segment_cost.

2.  If the condition in @eq-bin_seg_condition is true for at least one $\tau \in 1, \dots, n$, then the $\tau$ that minimizes $\mathcal{L}(y_{1:\tau}) + \mathcal{L}(y_{\tau+1:n})$ is picked as a first changepoint and the test is then performed on the two newly generated splits. This step is repeated until no further changepoints are detected on all resulting segments.

3.  If there are no more resulting valid splits, then the procedure ends.

Some of you might have noted how the condition in @eq-bin_seg_condition is closely related to the LR test in @eq-lr-test. In fact, rearranging equation above, gives us:

$$
- \mathcal{L}(y_{1:n}) + \mathcal{L}(y_{1:\tau}) + \mathcal{L}(y_{\tau+1:n}) = - \frac{LR_\tau}{2}  < -\beta.
$$

The $-\beta$ acts exactly as the constant $c$ for declaring a change, and it adds a natural stopping condition, solving the issue of overfitting that we mentioned in the previous section! Binary Segmentation, in fact, does nothing more then iteratively running a LR test, until no changes are found anymore!

This gives us a strategy to essentially apply a test that is locally optimal for one change, such as the Likelihood Ratio test, to solve a multiple changepoint segmentation. For this reason, BS is often employed to extend single changepoint procedures to multiple changes procedures, and hence it is one of the most prominent methods in the literature.

### Binary Segmentation in action

Having introduced the main idea, we show now how binary segmentation works in action with an example above. Say that we set a $\beta = 2 \log(400) =$ `{r} round(2 * log(400), 2)`.

**Step 1:** We start by computing the cost as in @eq-bin_seg_condition, and for those that are less then $\beta$, we pick the smallest. This will be our first changepoint estimate, and the first point of split.

In the plots below, the blue horizontal line is the mean signal estimated for a given split, while in the cusum the pink will represent the values of the LR below the threshold $\beta$, and red vertical line will show the min of the test statistics. When the cost is below the beta line, this will be our changepoint estimate.

In our case, we can see that the min of our cost has been achieved for $\hat\tau=100$, and since this is below the threshold, it's our first estimated changepoint!

```{r echo=F}
beta =  round(2 * log(400), 2)

cusum_plot <- function(y, low) {
  # Calculate CUSUM statistics
cusum_results <- LR(y)

n <- length(y)

mu <- c(rep(mean(y[1:cusum_results$tau]), cusum_results$tau),
  rep(mean(y[(cusum_results$tau+1):n]), n - cusum_results$tau))

# Create a data frame for the CUSUM trace
cusum_df <- data.frame(
  x = 1:(n - 1) + low,
  y = y[1:(n - 1)],
  mu = mu[1:(n - 1)],
  LR = - cusum_results$LR / 2
)

# Plot 1: Time series
p1 <- ggplot(cusum_df, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = mu), col="blue") + 
  theme_minimal()

# Plot 2: CUSUM trace
p2 <- ggplot(cusum_df, aes(x = x, y = LR)) +
  geom_line() +
  geom_line(data = cusum_df |> filter(LR < -beta), aes(x = x, y = LR), col = "pink") +
  geom_hline(yintercept = -beta, col="pink") + 
  geom_vline(xintercept = cusum_results$tau + low, col="red") +
  theme_minimal()

combined_plot <- ggarrange(p1, p2, align = "hv", nrow = 2)
return(combined_plot)
}

cusum_plot(y, 1)
```

**Step 2:**

From the first step, we have to check now two splits:

-   The first left split, **1-LEFT** in the plot below, covers data $y_{1:100}$. We can see that from here, the min of our statistic is below the threshold, therefore we won't declare any further change in this subset.

-   The first right split, **1-RIGHT** covers data $y_{101:400}$. We can see that here, the min of the statistics, is below the threshold, and therefore we identify a second change at $\hat\tau = 297$. This is not exactly 300, so we don't have a perfect estimate. Despite this is not ideal, this is the best point we have found and therefore we have to continue!

```{r echo=F}
ggarrange(cusum_plot(y[1:100], 1), cusum_plot(y[101:400], 101), labels = c("1-LEFT", "1-RIGHT"), widths = c(1.5, 2.5))
```

**Step 3:**

In step 3, we have to check again two splits splits:

-   The second left split, **2-LEFT** in the plot below, covers data $y_{101:297}$. Now, it's in this split that the statistics goes below the threshold! The third estimated change is at $\hat\tau = 203$, again slightly off the real one at 200. We continue investigating this split...

-   The second right split, **2-RIGHT** covers data $y_{298:400}$. In this last split, the min is not over the threshold, therefore we stop the search.

```{r echo=F}
ggarrange(cusum_plot(y[101:297], 100), cusum_plot(y[298:400], 301), labels = c("2-LEFT", "2-RIGHT"), widths = c(2.7, 1.3))
```

**Step 4:**

In step 4, we check:

-   The third left split, **3-LEFT** in the plot below, covers data $y_{101:203}$. The minimum, in here is not over the threshold.

-   The third right split, **3-RIGHT** covers data $y_{204:298}$. Similarly, the minimum is not over the treshold.

```{r echo=F}
ggarrange(cusum_plot(y[101:203], 100), cusum_plot(y[204:298], 203), labels = c("3-LEFT", "3-RIGHT"))
```

The algorithm therefore terminates!

With this graphical description in mind, we formally describe the Binary Segmentation algorithm as a recursive procedure, where the first iteration would be simply given by $\text{BinSeg}(y_{1:n}, \beta)$.

------------------------------------------------------------------------

| $\text{BinSeg}(y_{s:t}, \beta)$
| 

------------------------------------------------------------------------

| **INPUT:** Subseries $y_{s:t} = \{y_s, \dots, y_t\}$ of length $t - s + 1$, penalty $\beta$
| **OUTPUT:** Set of detected changepoints $cp$
| 
| **IF** $t - s \leq 1$
|     **RETURN** $\{\}$ // No changepoint in segments of length 1 or less
| 
| **COMPUTE**
| $\mathcal{Q} \leftarrow \underset{\tau \in \{s, \dots, t\}}{\min} \left[ \mathcal{L}(y_{s:\tau}) + \mathcal{L}(y_{\tau+1:t}) - \mathcal{L}(y_{s:t}) + \beta \right]$
| 
| **IF** $\mathcal{Q} < 0$
|     $\hat{\tau} \leftarrow \underset{\tau \in \{s, \dots, t\}}{\text{arg}\min} \left[ \mathcal{L}(y_{s:\tau}) + \mathcal{L}(y_{\tau+1:t}) - \mathcal{L}(y_{s:t}) \right]$
|      $cp \leftarrow \{ \hat{\tau}, \text{BinSeg}(y_{s:\hat{\tau}}, \beta), \text{BinSeg}(y_{\hat{\tau}+1:t}, \beta) + \hat\tau \}$
|      **RETURN** $cp$
| 
| **RETURN** $\{\}$ // No changepoint if $-LR/2$ is above penalty $- \beta$

------------------------------------------------------------------------

## Optimal Partitioning

Another solution to avoid the over-fitting problem of @eq-segment_cost lies in introducing a penalty term that discourages too many changepoints, avoiding overfitting. This is known as the *penalised approach*.

To achieve this, we want to minimize the following cost function:

$$
Q_{n, \beta} = \min_{K \in \mathbb{N}} \left[ \min_{\substack{\\ \tau_1, \dots, \tau_K}} \sum_{k = 0}^K \mathcal{L}(y_{\tau_k+1:\tau_{k+1}}) + \beta K
 \right],
$$

where $Q_{n, \beta}$ represents the optimal cost for segmenting the data up to time $n$ with a penalty \$ \beta \$ that increases with each additional changepoint $K$. With the $\beta$ term, for every new changepoint added, the cost of the full segmentation increases, discouraging therefore models with too many changepoints.

Unlike Binary Segmentation, which works iteratively and makes local decisions about potential changepoints, and as we have seen it is prone to errors, solving $Q_{n, \beta}$ ensures that the segmentation is **globally optimal**, as in the location of the changes are the best possible to minimise our cost.

Now, directly solving this problem using a brute-force search is computationally prohibitive, as it would require checking every possible combination of changepoints across the sequence: the number of possible segmentations grows exponentially as $n$ increases...

Fortunately, this problem can be solved efficiently using a sequential, dynamic programming algorithm: **Optimal Partitioning (OP)**, from @Jackson. OP solves @eq-pen-cost exactly through the following recursion.

We start with $\mathcal{Q}_{0, \beta} = -\beta$, and then, for each $t = 1, \dots, n$, we compute:

$$
    \mathcal{Q}_{t, \beta} = \min_{0 \leq \tau < t} \left[ \mathcal{Q}_{\tau, \beta} + \mathcal{L}(y_{\tau + 1:t}) + \beta \right].
$$ {#eq-optimal-partitioning}

Here, $\mathcal{Q}_{t, \beta}$ represents the optimal cost of segmenting the data up to time $t$. The algorithm builds this solution sequentially by considering each possible segmentation $\mathcal{Q}_{0, \beta},\ \cdots, \mathcal{Q}_{t-2, \beta},\ \mathcal{Q}_{t-1, \beta}$ before the current time $t$, plus the segment cost up to current time $t$, $\mathcal{L}(y_{\tau + 1:t})$.

### Optimal partitinioning in action

This recursion can be quite hard to digest, and is, as usual, best described graphically.

**Step 1** Say we are at $t = 1$. In this case, according to equation above, the optimal cost up to time one will be given by (remember that the $\beta$ cancels out with $Q_{0, \beta}$!):

$$
 \mathcal{Q}_{1, \beta} = \left[ -\beta + \mathcal{L}(y_{1:1}) + \beta \right] = \mathcal{L}(y_{1:1})
$$

```{r, echo=FALSE}
set.seed(5)
x <- c(rnorm(6))
plot(x, type = "l", xlim = c(-1, 10), ylim = c(-12, 8.5), xlab= "t", ylab = " ", col = "grey", yaxt = "n")
points(1:6, x, pch = 19, cex = 1.5, col = "grey")
  
offset = -6
sep = 2
operations = 1
  
points(1, x[1], pch = 19, cex = 1.5)

# Right side - Cost of fitting a new change with dynamic tau and t
segments(x0 = 1 - .15, x1 = 1+0.25, y0 = 1 + offset, y1 = 1 + offset, lwd = 2, lty = 2)
# Dynamic label with tau = i and t = n
text(x = 1 + 1.3, y = 1 + offset, labels = bquote(L(y[.(1):.(1)])), cex = 0.8)

```

**Step 2.** Now, at the second step, we have to minimise between two segmentations:

-   One with the whole sequence in a second segment alone (again, $\beta$ cancels out with $Q_{0, \beta} = -\beta$), and this will be given by $\mathcal{L}(y_{1:2})$ (dotted line)
-   One with the optimal segmentation from step 1 $\mathcal{Q}_{1, \beta}$ (whose cost considered only the first point in its own segment!), to which we have to sum the cost relative to a second segment $\mathcal{L}(y_{2:2})$ that puts the second point alone, and the penalty $\beta$ as we have added a new segment!

We minimise across the two, and this gives us $Q_{2, \beta}$.

```{r, echo=FALSE}
op_plot <- function(t){
  set.seed(5)
  x <- c(rnorm(6))
  plot(x, type = "l", xlim = c(-1, 8.5), ylim = c(-13, 4), xlab= "t", ylab = " ", col = "grey", yaxt = "n")
  points(1:6, x, pch = 19, cex = 1.5, col = "grey")
  
  offset = -6
  sep = 2
  operations = 1
  
  points(1:t, x[1:t], pch = 19, cex = 1.5)
  
  # Right side - Cost of fitting a new change with dynamic tau and t
  segments(x0 = 1 - .15, x1 = t + 0.15, y0 = 3 + offset, y1 = 3 + offset, lwd = 2, lty = 2)
  # Dynamic label with tau = i and t = n
  text(x = t + 1.3, y = 3 + offset, labels = bquote(L(y[.(1):.(t)])), cex = 0.8)
  
  for (i in 1:(t-1)) {
    # Left side - Minimum cost of segmenting data up to time t (use dynamic i for Q_i)
    segments(x0 = 1 - .15, x1 = i + 0.15, y0 = 1 + offset, y1 = 1 + offset, lwd = 2)
    # Dynamic label with Q_i where i is the current index
    text(x = 0.3, y = 1 + offset, labels = bquote(Q[.(i)*beta]), cex = 0.8)
    
    text(x = i + 0.5, y = 1 + offset, labels = "+", cex = 0.8)
    
    # Right side - Cost of fitting a new change with dynamic tau and t
    segments(x0 = i + 1 - .15, x1 = t + 0.15, y0 = 1 + offset, y1 = 1 + offset, lwd = 2, lty = 2)
    # Dynamic label with tau = i and t = n
    text(x = t + 1.3, y = 1 + offset, labels = bquote(L(y[.(i+1):.(t)]) + beta), cex = 0.8)
    
    operations <- operations + 1
    offset = offset - sep
  }
}
op_plot(2)
```

*Step 3*: Similarly, at $t = 3$ we have now three segmentations to choose from:

-   The one that puts the first three observations in the same segment, whose cost will be given simply by $\mathcal{L}(y_{1:2})$,

-   The one considering the optimal segmentation from time 1, plus the cost of adding an extra segment with observation 2 and 3 together

-   Finally the optimal from segmentation 2, $\mathcal{Q}_{2, \beta}$, plus the segment cost of fitting an extra segment with point 3 alone. Note that $\mathcal{Q}_{2, \beta}$ will come from the step before: if we would have been beneficial to add a change, at the previous step, this information is carried over!

Again, we pick the minimum across these three to get $\mathcal{Q}_{3, \beta}$, and proceed.

```{r, echo=FALSE}
op_plot(3)
```

*Step* $n$ Until the last step! Which would look something like this:

```{r, echo=FALSE}
op_plot(6)
```

A formal description of the algorithm can be found below:

------------------------------------------------------------------------

| **INPUT:** Time series $y = (y_1, ..., y_n)$, penalty $\beta$
| **OUTPUT:** Optimal changepoint vector $cp_n$
| 
| Initialize $\mathcal{Q}_0 \leftarrow -\beta$
| Initialize $cp_0 \leftarrow \{\}$
| 
| **FOR** $t = 1, \dots, n$
|      $\mathcal{Q}_t \leftarrow \min_{0 \leq \tau < t} \left[ \mathcal{Q}_{\tau} + \mathcal{L}(y_{\tau + 1:t}) + \beta \right]$
|      $\hat\tau \leftarrow \text{arg}\min_{0 \leq \tau < t} \left[ \mathcal{Q}_{\tau} + \mathcal{L}(y_{\tau + 1:t}) + \beta \right]$
|      $cp_t \leftarrow (cp_\hat\tau, \hat\tau)$ // Append the changepoint to the list at the last optimal point
| 
| **RETURN** $cp_n$

------------------------------------------------------------------------

Running the Optimal Partitioning method on our example scenario, with the same penalty $\beta = 2 \log(400) =$ `{r} round(2 * log(400), 2)` as above, gives changepoint locations $\tau_{1:4} = \{100, 203, 301\}$.

```{r echo=FALSE}
library(changepoint)
op_out <- cpt.mean(y, penalty = "Manual", pen.value =  2 * log(400), method = "PELT")

plot(op_out, ylab="y")
```

So we can see how on this dataset in particular, OP performs slightly better then Binary Segmentation on the last change, getting closer to the real changepoint of 300!

## A Comparison of the two Approaches

When deciding which segmentation approach to use, Binary Segmentation (BS) and Optimal Partitioning (OP) each offer different strengths. The choice largely depends on the characteristics of the data and the goal of the analysis.

### Quality of the Segmentation

Generally, Optimal Partitioning (OP) provides the *most accurate segmentation*, especially when we have a well-defined model and expect precise changepoint detection. OP ensures that the solution is optimal by globally minimizing the cost function across all possible segmentations. This is ideal for datasets with clear changes, even if noise is present.

Let's consider a case with true changepoints at $\tau = 100, 200, 300$, and segment means $\mu_{1:4} = 2, 1, -1, 1.5$:

```{r echo=FALSE}
#| fig-height: 2
mu <- c(rep(2, 100), rep(1, 100), rep(-1, 100), rep(1.5, 100))

set.seed(27)
y <- mu + rnorm(400)
# Calculate CUSUM statistics
cusum_results <- LR(y)

# Create a data frame for the CUSUM trace
cusum_df <- data.frame(
  x = 1:(length(y) - 1),
  y = y[1:(length(y) - 1)],
  mu = mu[1:(length(mu) - 1)],
  CUSUM = cusum_results$LR
)

cs_res_tot <- cusum_df

# Plot 1: Time series
p1 <- ggplot(cusum_df, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = mu), col="red") + 
  theme_minimal()
p1
```

While the underlying signal follows these clear shifts, noise complicates segmentation. Binary Segmentation uses a greedy process where each iteration looks for the largest changepoint. Although fast, this local search can make mistakes if the signal isn't perfectly clear, particularly in the early stages of the algorithm. For example, running BS on this dataset introduces a mistake at $\tau = 136$, as shown in the plot below:

```{r echo=FALSE}
#| fig-height: 3
cusum_plot(y, 1)
```

This error is carried in the subsequent steps, and the full binary segmentation algorithm will output an additional change at $\tau = 136$... Optimal Partitioning (OP), on the other hand, evaluates all possible segmentations considers the overall fit across the entire sequence. It is therefore less susceptible to adding "ghost" changepoints, as rather than focusing on the largest change at each step.

To illustrate, we compare the segmentations generated by both approaches:

```{r echo=FALSE}

# Apply Binary Segmentation and Optimal Partitioning
out_bs <- cpt.mean(y, penalty = "Manual", pen.value = 2 * log(400), method = "BinSeg")
out_op <- cpt.mean(y, penalty = "Manual", pen.value = 2 * log(400), method = "PELT")

# Create a data frame for plotting
df <- data.frame(
  x = 1:400,
  y = y,
  true_mu = mu
)

bs_cpts <- cpts(out_bs)
op_cpts <- cpts(out_op)

# Plot data with changepoints
p <- ggplot(df, aes(x = x, y = y)) +
  geom_point(alpha = 0.6) +
  geom_line(aes(y = true_mu), color = "red", linetype = "dashed", size = 1.2) +
  
  # Add Binary Segmentation changepoints
  geom_vline(xintercept = bs_cpts, linetype = "dotted", color = "blue", size = 1.1) +
  annotate("text", x = bs_cpts, y = max(y), label = paste("BS:", bs_cpts), color = "blue", hjust = -0.1, vjust = 1) +
  
  # Add Optimal Partitioning changepoints
  geom_vline(xintercept = op_cpts, linetype = "solid", color = "green", size = 1.1) +
  annotate("text", x = op_cpts, y = min(y), label = paste("OP:", op_cpts), color = "green", hjust = -0.1, vjust = -1) +
  
  # Titles and theme
  labs(
    title = "Comparison of Binary Segmentation and Optimal Partitioning",
    x = "Time",
    y = "Signal"
  ) +
  theme_minimal()

# Create a data frame for plotting
df <- data.frame(
  x = 1:400,
  y = y,
  true_mu = mu
)

# Plot data with changepoints
p <- ggplot(df, aes(x = x, y = y)) +
  geom_point(alpha = 0.6) +
  geom_line(aes(y = true_mu), color = "red", linetype = "dashed") +
  
  # Add Binary Segmentation changepoints
  geom_vline(xintercept = bs_cpts, linetype = "dotted", color = "blue") +
  annotate("text", x = bs_cpts, y = max(y), label = paste(bs_cpts), color = "blue", hjust = -0.1, vjust = 1) +
  
  # Add Optimal Partitioning changepoints
  geom_vline(xintercept = op_cpts, linetype = "solid", color = "green") +
  annotate("text", x = op_cpts, y = min(y), label = paste(op_cpts), color = "green", hjust = -0.1, vjust = -1) +
  
  # Titles and theme
  labs(
    x = "Time") +
  theme_minimal()

p

```

### Computational Complexity

Well, you may ask why not using OP all the time, then? Well, in changepoint detection, in which is the most appropiate method, we often have to keep track of the computational performance too, and Binary Segmentation is faster on average. For this reason, for large datasets where approximate solutions are acceptable, it might be the best option.

Specifically:

-   **Binary Segmentation** starts by dividing the entire sequence into two parts, iteratively applying changepoint detection to each segment. In the average case, it runs in $\mathcal{O}(n \log n)$ because it avoids searching every possible split point. However, in the worst case (if all data points are changepoints), the complexity can degrade to $\mathcal{O}(n^2)$, as each step can require recalculating test statistics for a growing number of segments.

-   **Optimal Partitioning**, on the other hand, solves the changepoint problem by recursively considering every possible split point up to time $t$. The result is an optimal segmentation, but at the cost of $\mathcal{O}(n^2)$ computations. This holds true for both the average and worst cases, as it always requires a full exploration of all potential changepoints. However, as we will see in the tutorial and lab, there are *recent developments* that allow for a faster computation of OP, achieving average cases of $\mathcal{O}(n \log n)$: those are the **PELT** and **FPOP** algorithms.

### Neuroblastoma example

Good news is, on the neuroblastoma dataset, the two methods produce the same results!

```{r echo=FALSE}

one.dt <- nb.dt |> filter(profile.id==4, chromosome==2)

y <- one.dt$logratio
y <- (y)/mad(y)
n <- length(y)

# Apply Binary Segmentation and Optimal Partitioning
out_bs <- cpt.mean(y, method = "BinSeg")
out_op <- cpt.mean(y, method = "PELT")

# Create a data frame for plotting
df <- data.frame(
  x = 1:n,
  y = y
)

bs_cpts <- cpts(out_bs)
op_cpts <- cpts(out_op)

# Create a data frame for plotting
df <- data.frame(
  x = 1:n,
  y = y
)

# Plot data with changepoints
p <- ggplot(df, aes(x = x, y = y)) +
  geom_point(alpha = 0.6) +
  geom_vline(xintercept = bs_cpts, linetype = "dotted", color = "blue") +
  annotate("text", x = bs_cpts, y = max(y), label = paste(bs_cpts), color = "blue", hjust = -0.1, vjust = 1) +
    geom_vline(xintercept = op_cpts, linetype = "solid", color = "green") +
  annotate("text", x = op_cpts, y = min(y), label = paste(op_cpts), color = "green", hjust = -0.1, vjust = -1) +
  
  # Titles and theme
  labs(
    x = "Time") +
  theme_minimal()

p
```

## Exercises

### Workshop 3

In this workshop we will try to fix the two major drawbacks of OP and BS.

1.  In OP, we can reduce the numbers of checks to be performed at each iteration, reducing the complexity. This operation is called *pruning*. Specifically, on the condition that there exists a constant $\kappa$ such that for every $l < t < u$: $$
        \mathcal{L}(y_{l + 1:t}) + \mathcal{L}(y_{t + 1:u}) + \kappa \leq \mathcal{L}(y_{l + 1:u})
    $$ It is possible to *prune* without resorting to an approximation. For many cost functions, such as the Gaussian cost, such a constant exists. Equating $\kappa$ to the penalty $\beta$, gives us a computational trick to improve on the efficiency... The PELT algorithm -- acronym for Pruned Exact Linear Time -- (@Killick) solves exactly the penalised minimization of @eq-optimal-partitioning with an expected computational cost that can be linear in $n$ -- while still retaining $\mathcal{O}(n^2)$ computational complexity in the worst case. This is achieved by reducing the number of segment costs to evaluate at each iteration via an additional pruning step based on Condition @eq-optimal-partitioning. That is, if $$\mathcal{Q}\tau + \mathcal{L}(y_{\tau + 1:t}) + \kappa \geq \mathcal{Q}_t$$ then we can safely prune the segment cost related to $\tau$, as $\tau$ will never be the optimal changepoint location up to any time $T > t$ in the future. On the basis of this information, your task is to write the PELT algorithm. You can do this by changing the Optimal Partitioning algorithm, adding an additional step to reflect the PELT pruning.

2.  In BS, one of the issues that may arise, is an incorrect segmentation. WBS, @Fryzlewicz:2014, is a multiple changepoints procedures that improve on the BS changepoint estimation via computing the initial segmentation cost of BS multiple times over $M + 1$ random subsets of the sequence, $y_{s_1:t_1}, \dots, y_{s_M:t_M}, y_{1:n}$, picking the best subset according to what achieves the smallest segmentation cost and reiterating the procedure over that sample accordingly. The idea behind WBS lies in the fact that a favourable subset of the data $y_{s_m:t_m}$ could be drawn which contains a true change sufficiently separated from both sides $s_m, t_m$ of the sequence. By the inclusion of the $y_{1:n}$ entire sequence amongst the subsets, it is guaranteed that WBS will do no worse than the simple BS algorithm.

### Lab 3

1.  Code the Optimal Partitioning algorithm for the Gaussian change-in-mean case.\
    **Tips:**

    a.  You can pre-compute all the possible $\mathcal{L(y_{l:u})}$, for $u \geq l$ to save computational time, and avoiding recomputing all costs.

    b.   Be very careful with indexing... R starts indexing at 1, however, in the pseudocode, you have one element that starts at 0...

2.  Install the `changepoint` package. The example signal of Section 3.4, with the same penalty value of $\beta = 2 * log(n)$, compare your implementation of OP, Binary Segmentation, and PELT. You will learn how to do so in the documentation, running `?cpt_mean`. You should make 300 replicates each of your experiment, and compare:

    a.  The absolute error loss from the mean signal, e.g. $||\mu_{1:n} - \hat{\mu}_{1:n}||^2_2$

    b.  The absolute error in the number of changepoints reconstructed, e.g. $|K - \hat{K}|$. What do we observe?

    c.  The runtime for one sequence of PELT agains Binary Segmentation. Evaluate the runtime with package `microbenchmark`.
