# PELT, WBS and Penalty choices

```{r echo=FALSE, message=FALSE}
library(tidyverse)
library(ggpubr)
library(changepoint)
```

## Drawbacks of OP and BS

When deciding which segmentation approach to use, Binary Segmentation (BS) and Optimal Partitioning (OP) each offer different strengths. The choice largely depends on the characteristics of the data and the goal of the analysis.

### Quality of the Segmentation

Generally, Optimal Partitioning (OP) provides the *most accurate segmentation*, especially when we have a well-defined model and expect precise changepoint detection. OP ensures that the solution is optimal by globally minimizing the cost function across all possible segmentations. This is ideal for datasets with clear changes, even if noise is present.

Let's consider a case with true changepoints at $\tau = 100, 200, 300$, and segment means $\mu_{1:4} = 2, 1, -1, 1.5$:

```{r echo=FALSE}

cusum_plot <- function(y, low) {
  # Calculate CUSUM statistics
cusum_results <- LR(y)

n <- length(y)

mu <- c(rep(mean(y[1:cusum_results$tau]), cusum_results$tau),
  rep(mean(y[(cusum_results$tau+1):n]), n - cusum_results$tau))

# Create a data frame for the CUSUM trace
cusum_df <- data.frame(
  x = 1:(n - 1) + low,
  y = y[1:(n - 1)],
  mu = mu[1:(n - 1)],
  LR = - cusum_results$LR / 2
)

# Plot 1: Time series
p1 <- ggplot(cusum_df, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = mu), col="blue") + 
  theme_minimal()

# Plot 2: CUSUM trace
p2 <- ggplot(cusum_df, aes(x = x, y = LR)) +
  geom_line() +
  geom_line(data = cusum_df |> filter(LR < -beta), aes(x = x, y = LR), col = "pink") +
  geom_hline(yintercept = -beta, col="pink") + 
  geom_vline(xintercept = cusum_results$tau + low, col="red") +
  theme_minimal()

combined_plot <- ggarrange(p1, p2, align = "hv", nrow = 2)
return(combined_plot)
}
LR <- function(x) {
  #input is data vector length >=2
  S <- cumsum(x) #calculate cummulate sum of data
  n <- length(x) #number of data points
  tau <- 1:(n - 1) #possible change-point locations to test
  D <- S[tau] / tau - (S[n] - S[tau]) / (n - tau) #difference in means
  LR <- D ^ 2 * tau * (n - tau) / n #LR statistic at locations tau
  #return LR statistic and estimate of tau
  return(list(LR = LR, LR_max = max(LR), tau.hat = which.max(LR)))
}

beta <- round(2 * log(400), 2)

#| fig-height: 2
mu <- c(rep(2, 100), rep(1, 100), rep(-1, 100), rep(1.5, 100))

set.seed(27)
y <- mu + rnorm(400)
# Calculate CUSUM statistics
cusum_results <- LR(y)

# Create a data frame for the CUSUM trace
cusum_df <- data.frame(
  x = 1:(length(y) - 1),
  y = y[1:(length(y) - 1)],
  mu = mu[1:(length(mu) - 1)],
  CUSUM = cusum_results$LR
)

cs_res_tot <- cusum_df

# Plot 1: Time series
p1 <- ggplot(cusum_df, aes(x = x, y = y)) +
  geom_point() +
  geom_line(aes(x = x, y = mu), col="red") + 
  theme_minimal()
p1
```

While the underlying signal follows these clear shifts, noise complicates segmentation. Binary Segmentation uses a greedy process where each iteration looks for the largest changepoint. Although fast, this local search can make mistakes if the signal isn't perfectly clear, particularly in the early stages of the algorithm. For example, running BS on this dataset introduces a mistake at $\tau = 136$, as shown in the plot below:

```{r echo=FALSE}
#| fig-height: 3
cusum_plot(y, 1)
```

This error is carried in the subsequent steps, and the full binary segmentation algorithm will output an additional change at $\tau = 136$... Optimal Partitioning (OP), on the other hand, evaluates all possible segmentations considers the overall fit across the entire sequence. It is therefore less susceptible to adding "ghost" changepoints, as rather than focusing on the largest change at each step.

To illustrate, we compare the segmentations generated by both approaches:

```{r echo=FALSE, warning=FALSE}

# Apply Binary Segmentation and Optimal Partitioning
out_bs <- cpt.mean(y, penalty = "Manual", pen.value = 2 * log(400), method = "BinSeg")
out_op <- cpt.mean(y, penalty = "Manual", pen.value = 2 * log(400), method = "PELT")

# Create a data frame for plotting
df <- data.frame(
  x = 1:400,
  y = y,
  true_mu = mu
)

bs_cpts <- cpts(out_bs)
op_cpts <- cpts(out_op)

# Plot data with changepoints
p <- ggplot(df, aes(x = x, y = y)) +
  geom_point(alpha = 0.6) +
  geom_line(aes(y = true_mu), color = "red", linetype = "dashed", size = 1.2) +
  
  # Add Binary Segmentation changepoints
  geom_vline(xintercept = bs_cpts, linetype = "dotted", color = "blue", size = 1.1) +
  annotate("text", x = bs_cpts, y = max(y), label = paste("BS:", bs_cpts), color = "blue", hjust = -0.1, vjust = 1) +
  
  # Add Optimal Partitioning changepoints
  geom_vline(xintercept = op_cpts, linetype = "solid", color = "green", size = 1.1) +
  annotate("text", x = op_cpts, y = min(y), label = paste("OP:", op_cpts), color = "green", hjust = -0.1, vjust = -1) +
  
  # Titles and theme
  labs(
    title = "Comparison of Binary Segmentation and Optimal Partitioning",
    x = "Time",
    y = "Signal"
  ) +
  theme_minimal()

# Create a data frame for plotting
df <- data.frame(
  x = 1:400,
  y = y,
  true_mu = mu
)

# Plot data with changepoints
p <- ggplot(df, aes(x = x, y = y)) +
  geom_point(alpha = 0.6) +
  geom_line(aes(y = true_mu), color = "red", linetype = "dashed") +
  
  # Add Binary Segmentation changepoints
  geom_vline(xintercept = bs_cpts, linetype = "dotted", color = "blue") +
  annotate("text", x = bs_cpts, y = max(y), label = paste(bs_cpts), color = "blue", hjust = -0.1, vjust = 1) +
  
  # Add Optimal Partitioning changepoints
  geom_vline(xintercept = op_cpts, linetype = "solid", color = "green") +
  annotate("text", x = op_cpts, y = min(y), label = paste(op_cpts), color = "green", hjust = -0.1, vjust = -1) +
  
  # Titles and theme
  labs(
    x = "Time") +
  theme_minimal()

p

```

### Computational Complexity

Well, you may ask why not using OP all the time, then? Well, in changepoint detection, in which is the most appropiate method, we often have to keep track of the computational performance too, and Binary Segmentation is faster on average. For this reason, for large datasets where approximate solutions are acceptable, it might be the best option.

Specifically:

-   **Binary Segmentation** starts by dividing the entire sequence into two parts, iteratively applying changepoint detection to each segment. In the average case, it runs in $\mathcal{O}(n \log n)$ because it avoids searching every possible split point. However, in the worst case (if all data points are changepoints), the complexity can degrade to $\mathcal{O}(n^2)$, as each step can require recalculating test statistics for a growing number of segments.

-   **Optimal Partitioning**, on the other hand, solves the changepoint problem by recursively considering every possible split point up to time $t$. The result is an optimal segmentation, but at the cost of $\mathcal{O}(n^2)$ computations. This holds true for both the average and worst cases, as it always requires a full exploration of all potential changepoints.

## PELT and WBS

Good news is, despite both algorithms have drawbacks, following *recent developments*, those have been solved. In the next sections, we will introduce two new algorithms, PELT and WBS.

### PELT: an efficient solution to OP

In OP, we can reduce the numbers of checks to be performed at each iteration, reducing the complexity. This operation is called *pruning*. Specifically, on the condition that there exists a constant $\kappa$ such that for every $l < t < u$:

$$
        \mathcal{L}(y_{l + 1:t}) + \mathcal{L}(y_{t + 1:u}) + \kappa \leq \mathcal{L}(y_{l + 1:u})
$$

It is possible to *prune* without resorting to an approximation. For many cost functions, such as the Gaussian cost, such a constant exists. Equating $\kappa$ to the penalty $\beta$, gives us a computational trick to improve on the efficiency... The PELT algorithm -- acronym for Pruned Exact Linear Time -- (@Killick) solves exactly the penalised minimization of @eq-optimal-partitioning with an expected computational cost that can be linear in $n$ -- while still retaining $\mathcal{O}(n^2)$ computational complexity in the worst case. This is achieved by reducing the number of segment costs to evaluate at each iteration via an additional pruning step based on Condition @eq-optimal-partitioning. That is, if $$\mathcal{Q}\tau + \mathcal{L}(y_{\tau + 1:t}) + \kappa \geq \mathcal{Q}_t$$ then we can safely prune the segment cost related to $\tau$, as $\tau$ will never be the optimal changepoint location up to any time $T > t$ in the future.

The intuition, is that, when $\kappa = \beta$, our penalty, then we would prune at every change detected. And if the changes increase linearly with the length of the data, this means that our algorithm will achieve a $\mathcal{O}(n \log n)$ computational complexity, without any drawbacks!

![](source_imgs/OPPELT.png)

To reduce computational complexity, we can slightly modify the OP algorithm, to add the pruning condition above:

|      |
|:-----|
| PELT |

| **INPUT:** Time series $y = (y_1, ..., y_n)$, penalty $\beta$
| **OUTPUT:** Optimal changepoint vector $cp_n$
| 
| Initialize $\mathcal{Q}_0 \leftarrow -\beta$
| Initialize $cp_0 \leftarrow \{\}$
| Initialise $R_1 = \{0\}$
| 
| **FOR** $t = 1, \dots, n$
|      $\mathcal{Q}_t \leftarrow \min_{\tau \in R_t} \left[ \mathcal{Q}_{\tau} + \mathcal{L}(y_{\tau + 1:t}) + \beta \right]$
|      $\hat\tau \leftarrow \text{arg}\min_{\tau \in R_t} \left[ \mathcal{Q}_{\tau} + \mathcal{L}(y_{\tau + 1:t}) + \beta \right]$
|      $cp_t \leftarrow (cp_\hat\tau, \hat\tau)$ // Append the changepoint to the list at the last optimal point
|      $R_{t+1} \leftarrow \{\tau \in \{R_t \cup \{t\}\} : \mathcal{Q}_\tau + \mathcal{L}(y_{\tau + 1:t}) + \beta \leq \mathcal{Q}_t \}$ // prune the non-optimal changepoint locations
| 
| **RETURN** $cp_n$

------------------------------------------------------------------------

As the segmentation retained is effectively the same, there are literally no disadvantages in using PELT over OP, if the cost function allows to do so. The problem is that this is not the case with some cost functions, such as the one for the change-in-slope. There, in fact, we have that the cost from the next change depends on the location of the previous one, violating the condition above. 


### WBS: Improving on Binary Segmentation

In BS, one of the issues that may arise, is an incorrect segmentation. WBS, @Fryzlewicz:2014, is a multiple changepoints procedures that improve on the BS changepoint estimation via computing the initial segmentation cost of BS multiple times over $M + 1$ random subsets of the sequence, $y_{s_1:t_1}, \dots, y_{s_M:t_M}, y_{1:n}$, picking the best subset according to what achieves the smallest segmentation cost and reiterating the procedure over that sample accordingly. The idea behind WBS lies in the fact that a favorable subset of the data $y_{s_m:t_m}$ could be drawn which contains a true change sufficiently separated from both sides $s_m, t_m$ of the sequence. By the inclusion of the $y_{1:n}$ entire sequence among the subsets, it is guaranteed that WBS will do no worse than the simple BS algorithm.

We can formally provide a description of WBS as a recursive procedure again, just adding a couple of alterations to the original Binary Segmentation:

------------------------------------------------------------------------

| $\text{WBS}(y_{s:t}, \beta)$
| 

------------------------------------------------------------------------

| **INPUT:** Subseries $y_{s:t} = \{y_s, \dots, y_t\}$ of length $t - s + 1$, penalty $\beta$
| **OUTPUT:** Set of detected changepoints $cp$
| 
| **IF** $t - s \leq 1$
|     **RETURN** $\{\}$ // No changepoint in segments of length 1 or less
| 
| Draw $\mathcal{M} = \{ [s_1, t_1], \dots, [s_M, t_M] \}$ tuples of subset indexes;
| $\mathcal{M} \leftarrow \mathcal{M} \cup \{[1, n]\}$ 
| **COMPUTE**
| $\mathcal{Q} \leftarrow \underset{\substack{[s_m, t_m] \in \mathcal{M}\\ \tau \in \{s_m, \dots, t_m\}}}{\min} \left[ \mathcal{L}(y_{s:\tau}) + \mathcal{L}(y_{\tau+1:t}) - \mathcal{L}(y_{s:t}) + \beta \right]$
| 
| **IF** $\mathcal{Q} < 0$
|     $\hat{\tau} \leftarrow \underset{\substack{[s_m, t_m] \in \mathcal{M}\\ \tau \in \{s_m, \dots, t_m\}}}{\text{arg}\min} \left[ \mathcal{L}(y_{s:\tau}) + \mathcal{L}(y_{\tau+1:t}) - \mathcal{L}(y_{s:t}) \right]$
|      $cp \leftarrow \{ \hat{\tau}, \text{WBS}(y_{s:\hat{\tau}}, \beta), \text{WBS}(y_{\hat{\tau}+1:t}, \beta) + \hat\tau \}$
|      **RETURN** $cp$
| 
| **RETURN** $\{\}$ // No changepoint if $-LR/2$ is above penalty $- \beta$

------------------------------------------------------------------------

One of the major drawbacks of WBS is that in scenarios where we find frequent changepoints, in order to retain a close-to-optimal estimation, one should draw a higher number of $M$ intervals: this can be problematic given that WBS has computational complexity that grows linearly in the total length of the observations of the subsets.

## Penalty selection

### Mean absolute deviation

### CROPS
