[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MATH337: Changepoint Detection",
    "section": "",
    "text": "Preface\nThese are the notes for MATH337 Changepoint Detection. They were written by Gaetano Romano.\nThe module will introduce you to changepoint detection, detailing some algorithms, developing the basics theoretical foundations, and practicing few real-world scenarios.\nAcross five weeks we will cover the following topics:\n\nAn introduction to changepoint detection and the CUSUM statistics\nControlling the CUSUM and some additional models\nDealing with multiple changes\nPenalty selection\nBreaking the real-world assumptions.\n\nWe will be using R as the programming language for this module. If you’re unfamiliar with it, make sure you cover the first three weeks of MATH245.\nEvery week, you are expected to follow two lectures, one lab, and one computer aided workshop. Over the lecture, we will cover the basics concepts of changepoint detection. During the labs, you will be dealing with computations and details at your own pace, and, finally, during the workshops, you’ll give a go at programming the various algorithms and running real-world examples.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Baranowski, Rafal, Yining Chen, and Piotr Fryzlewicz. 2019.\n“Narrowest-over-Threshold Detection of Multiple Change Points and\nChange-Point-Like Features.” Journal of the Royal Statistical\nSociety Series B: Statistical Methodology 81 (3): 649–72.\n\n\nBown, Jonathan. 2023. “Simpsons Episodes & Ratings\n(1989-Present).” https://www.kaggle.com/datasets/jonbown/simpsons-episodes-2016?resource=download.\n\n\nFryzlewicz, Piotr. 2014. “Wild binary\nsegmentation for multiple change-point detection.”\nAnnals of Statistics 42: 2243–81.\n\n\nJackson, Brad, Jeffrey Scargle, D. Barnes, S. Arabhi, A. Alt, P.\nGioumousis, E. Gwin, P. Sangtrakulcharoen, L. Tan, and Tun Tsai. 2005.\n“An Algorithm for Optimal Partitioning of Data on an\nInterval.” Signal Processing Letters, IEEE 12 (March):\n105–8. https://doi.org/10.1109/LSP.2001.838216.\n\n\nKillick, R., P. Fearnhead, and I. A. Eckley. 2012. “Optimal\nDetection of Changepoints with a Linear Computational Cost.”\nJournal of the American Statistical Association 107 (500):\n1590–98.\n\n\nYao, Yi-Ching, and Richard A Davis. 1986. “The Asymptotic Behavior\nof the Likelihood Ratio Statistic for Testing a Shift in Mean in a\nSequence of Independent Normal Variates.”\nSankhyā: The Indian Journal of Statistics, Series\nA, 339–53.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "1_intro_cusum.html",
    "href": "1_intro_cusum.html",
    "title": "1  An Introduction to Changepoint Detection",
    "section": "",
    "text": "1.1 Piecewise Stationary Time Series\nIn this module, we will be dealing with time series. A time series is a sequence of observations recorded over time (or space), where the order of the data points is crucial.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Changepoint Detection</span>"
    ]
  },
  {
    "objectID": "2_control.html",
    "href": "2_control.html",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "",
    "text": "2.1 The asymptotic distribution of the CUSUM statistics\nWe have learned that the chi-squared distribution is a continuous probability distribution that models the sum of squares of k independent standard normal random variables. More formally, if \\(z_1, \\cdots, z_k\\) are independent, standard Normal random variables, then:\n\\[\n\\sum_{i=1}^k z^2_i \\sim \\chi^2_k\n\\]\nWe have met this distribution already in hypothesis testing and constructing confidence intervals. The shape of the distribution depends on its degrees of freedom (k). For \\(k=1\\), it’s highly skewed, but as k increases, it becomes more symmetric and approaches a normal distribution.\nWe learned that, when properly normalized \\(C_\\tau\\) follows a standard normal distribution under the null hypothesis of no change. Therefore, our test statistics for a fixed \\(\\tau\\), \\(C_\\tau^2/\\sigma^2\\), follows a chi-squared distribution with 1 degree of freedom, being the square of a standard random variable. If we take the example of last week, and remove the changepoint, we can observe that the cusum statistics stays constant, and relatively small:\nHowever, as the change is unknown, our actual test statistic for detecting a change is \\(\\max_\\tau C_\\tau^2/σ^2\\).\nFor this reason, calculating the distribution of this maximum ends up being a bit more challenging…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "1_intro_cusum.html#piecewise-constant-time-series",
    "href": "1_intro_cusum.html#piecewise-constant-time-series",
    "title": "1  An Introduction to Changepoint Detection",
    "section": "",
    "text": "1.1.1 What is a time series?\nIn previous modules, such as Likelihood Inference, we typically dealt with data that was not ordered in a particular way. For example, we might have worked with a sample of independent Gaussian observations, where each observation is drawn randomly from the same distribution. This sample might look like the following:\n\\[\n  y_i \\sim \\mathcal{N}(0,1), \\ i = 1, \\dots, 100\n\\]\nHere, \\(y_i\\) represents the \\(i\\)-th observation, and the assumption is that all observations are independent and identically distributed (i.i.d.) with a mean of 0 and variance of 1.\n\n\n\n\n\n\n\n\n\nIn this case, the observations do not have any particular order, and our primary interest may be in estimating parameters such as the mean, variance, or mode of the distribution. This is typical for traditional inference, where the order of observations is not of concern.\nHowever, a time series involves a specific order to the data—usually indexed by time, although it could also be by space or another sequential dimension. For example, we could assume that the Gaussian sample above is a sequential process, ordered by the time we drew an observation. Each observation corresponds to a specific time point \\(t\\). If the time points are indexed from 1 to 100, we would write the time series as:\n\\[\n  y_{1:100} = (y_1, y_2, \\dots, y_{100})\n\\]\nwhere \\(y_t\\) represents the observation at time \\(t\\), with, in case of our example, \\(t = 1, 2, \\dots, 100\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Changepoint Detection</span>"
    ]
  },
  {
    "objectID": "1_intro_cusum.html#piecewise-stationary-time-series",
    "href": "1_intro_cusum.html#piecewise-stationary-time-series",
    "title": "1  An Introduction to Changepoint Detection",
    "section": "",
    "text": "1.1.1 What is a time series?\nIn previous modules, such as Likelihood Inference, we typically dealt with data that was not ordered in a particular way. For example, we might have worked with a sample of independent Gaussian observations, where each observation is drawn randomly from the same distribution. This sample might look like the following:\n\\[\n  y_i \\sim \\mathcal{N}(0,1), \\ i = 1, \\dots, 100\n\\]\nHere, \\(y_i\\) represents the \\(i\\)-th observation, and the assumption is that all observations are independent and identically distributed (i.i.d.) with a mean of 0 and variance of 1.\n\n\n\n\n\n\n\n\n\nIn this case, the observations do not have any particular order, and our primary interest may be in estimating parameters such as the mean, variance, or mode of the distribution. This is typical for traditional inference, where the order of observations is not of concern.\nHowever, a time series involves a specific order to the data—usually indexed by time, although it could also be by space or another sequential dimension. For example, we could assume that the Gaussian sample above is a sequential process, ordered by the time we drew an observation. Each observation corresponds to a specific time point \\(t\\):\n\n\n\n\n\n\n\n\n\nFormal Notation. In time series analysis, we typically denote a time series by using an index \\(t\\) to represent time or order. The time series vector is written as:\n\\[\n  y_{1:n} = (y_1, y_2, \\dots, y_n)\n\\]\nHere, \\(n\\) is the total length of the sequence, and \\(y_t\\) represents the observed value at time \\(t\\), for \\(t = 1, 2, \\dots, n\\). In our previous example, for instance, \\(n = 100\\).\nOften, we are also interested in subsets of a time series (inclusive), especially when investigating specific “windows” or “chunks” of the data. We will denote as a subset of the time series, from time \\(l\\) to time \\(u\\), the following:\n\\[\n  y_{l:u} = (y_l, y_{l+1}, \\dots, y_u)\n\\]\nUnderstanding and working with subsets of time series data is important for many applications, such as when detecting changes in the behavior or properties of the time series over specific intervals.\n\n\n1.1.2 Stationary, non-stationary, and piecewise stationary time series\nTime series can be classified into different categories based on their statistical properties over time. The three main types are stationary, non-stationary, and piecewise stationary time series. For example:\n\n\n\n\n\n\n\n\n\n\nStationary Time Series: A time series is said to be stationary if its statistical properties—such as the mean, variance, and autocovariance—are constant over time. This implies that the behavior of the series doesn’t change as time progresses.\n\nMathematically, for a stationary time series \\(y_t\\), the expected value and variance are constant over time: \\[\n    \\mathbb{E}(y_t) = \\mu \\quad \\text{and} \\quad \\text{Var}(y_t) = \\sigma^2 \\quad \\forall \\in \\{1, ..., n\\}\n\\] In this example, the stationary time series was generated by sampling random normal variables \\(y_t = \\epsilon_t, \\ \\epsilon_t \\sim \\mathcal{N}(0, 1)\\). We can see, very simply how, in this case: \\[\n    \\mathbb{E}(y_t) = \\mathbb{E}(\\epsilon_t) = 0, \\forall t \\in \\{1, ..., 100\\}\n\\]\n\nNon-Stationary Time Series: A time series is non-stationary if its statistical properties change over time. Often, non-stationary series exhibit trends or varying variances. For example, a series with a trend (increasing or decreasing) is non-stationary because the mean is not constant.\n\nA common form of non-stationarity is a linear trend, where the series grows over time. In our example, the non-stationary series is generated as: \\[\n    y_t = \\epsilon_t + 0.1 \\cdot t , \\ \\epsilon_t \\sim \\mathcal{N}(0, 1)\n\\] This creates a time series with a linear upward trend. In fact, similarly to what done before: \\[\n    \\mathbb{E}(y_t) = \\mathbb{E}(\\epsilon_t) + \\mathbb{E}(0.1 \\cdot t) = 0.1 \\cdot t.\n\\] Therefore: \\[\n  \\forall t_1, t_2 \\in \\{1, ..., 100\\}, t_1 \\neq t_2 \\rightarrow \\mathbb{E}(y_{t_1}) \\neq \\mathbb{E}(y_{t_2})\n\\]\n\nPiecewise Stationary Time Series: A piecewise stationary time series is stationary within certain segments but has changes in its statistical properties at certain points, known as changepoints. After each changepoint, the series may have a different mean, variance, or both.\n\nIn our example, the time series was stationary for the first half of the observations, but after \\(t = 50\\), a sudden shift occurs. Mathematically: \\[\ny_t = \\begin{cases}\n    \\epsilon_t & \\text{for } t \\leq 50 \\\\\n    \\epsilon_t + 5 & \\text{for } t &gt; 50\n    \\end{cases}, \\quad \\epsilon_t \\sim \\mathcal{N}(0, 1)\n\\] This abrupt change at \\(t = 50\\) introduces a piecewise structure to the data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Changepoint Detection</span>"
    ]
  },
  {
    "objectID": "1_intro_cusum.html#introduction-to-changepoints",
    "href": "1_intro_cusum.html#introduction-to-changepoints",
    "title": "1  An Introduction to Changepoint Detection",
    "section": "1.2 Introduction to changepoints",
    "text": "1.2 Introduction to changepoints\nChangepoints are sudden, and often unexpected, shifts in the behavior of a process. They are also known as breakpoints, structural breaks, or regime switches. The detection of changepoints is crucial in understanding and responding to changes in various types of time series data.\nThe primary objectives in detecting changepoints include:\n\nHas a change occurred?: Identifying if there is a shift in the data.\nIf yes, where is the change?: Locating the precise point where the change happened.\nWhat is the difference between the pre and post-change data? This may reveal the type of change, and it could indicate differences in parameter values before and after the change.\nHow certain are we of the changepoint location?: Assessing the confidence in the detected changepoint.\nHow many changes have occurred?: Identifying multiple changepoints and analyzing each one for similar characteristics.\n\nChangepoints can be found in a wide range of time series, not limited to physical, biological, industrial, or financial processes, and which objectives to follow depends on the type of the analysis we are carrying.\nIn changepoint detection, there are two main approaches: online and offline analysis. In applications that require online analysis, the data is processed as it arrives, or in small batches. The primary goal of online changepoint detection is to identify changes as quickly as possible, making it crucial in contexts such as process control or intrusion detection, where immediate action is necessary.\nOn the other hand, offline analysis processes all the data at once, typically after it has been fully collected. The aim here is to provide an accurate detection of changepoints, rather than a rapid one. This approach is common in fields like genome analysis or audiology, where the focus is on understanding the structure of the data post-collection.\nFor instance, to give few examples:\n\nECG: Detecting changes or abnormalities in electrocardiogram (ECG) data can help in diagnosing heart conditions.\n\n\n\nElectrocardiograms (heart monitoring), Fotoohinasab et al, Asilomar conference 2020.\n\n\nCancer Diagnosis: Identifying breakpoints in DNA copy number data is important for diagnosing some types of cancer, such as neuroblastoma. This is a typical example of an offline analysis.\n\n\n\nDNA copy number data, breakpoints associated with aggressive cancer, Hocking et al, Bioinformatics 2014.\n\n\nEngineering Monitoring: Detecting changes in CPU monitoring data in servers can help in identifying potential issues or failures: this is often analysed in real-time on with online methods, with the aim of detecting an issue as quickly as possible.\n\n\n\nTemperature data from a CPU of an AWS server. Source Romano et al., (2023)\n\n\n\nIn this module, we will focus exclusively on offline changepoint detection, where we assume that all the data is available for analysis from the start.\n\n1.2.1 Types of Changes in Time Series\nAs you might have noticed from the examples above, there’s not a strict way on which a time-series might change. Depending on the model, we could seek for different types of changes in the structure of a time series. Some of the most common types of changes include shifts in mean, variance, and trends in regression. For example, the CPU example above exihibited, in addition to some extreme observations, both changes in mean and variance.\n\nA change in mean occurs when the average level of an otherwise stationary time series shifts from one point to another. This type of change is often encountered in real-world data when there is a sudden shift in the process generating the data, such as a change in policy, market conditions, or external factors affecting the system.\n\n\n\n\n\n\n\n\n\n\nIn the plot above, the red lines indicate the true mean values of the different segments.\n\nA change in variance refers to a shift in the variability of the time series data, even when the mean remains constant. This type of change is important in scenarios where the stability of a process fluctuates over time. For example, in financial markets, periods of high volatility (high variance) may be followed by periods of relative calm (low variance).\n\n\n\n\n\n\n\n\n\n\n\n1.2.1.1 3. Change in Regression (Slope)\nA change in regression or slope occurs when the underlying relationship between time and the values of the time series changes. This could reflect a shift in the growth or decline rate of a process. For example, a company’s revenue might grow steadily over a period, then plateau, and later exhibit a quadratic or nonlinear growth trend.\n\n\n\n\n\n\n\n\n\n\n\n\n1.2.2 The biggest data challenge in changepoint detection\nOne of the most widely debated and difficult data challenges in changepoint detection may not be in the field of finance, genetics, or climate science—but rather in television history. Specifically, the question that has plagued critics and fans alike for years is: At which episode did “The Simpsons” start to decline?\nIt’s almost common knowledge that “The Simpsons,” the longest-running and most beloved animated sitcom, experienced a significant drop in quality over time. But pinpointing exactly when this drop occurred is the real challenge. Fortunately, there’s a branch of statistics that was practically built to answer questions like these!\n\nI have downloaded a dataset (Bown 2023) containing ratings for every episode of “The Simpsons” up to season 34. We will analyze this data to determine if and when a significant shift occurred in the ratings, which might reflect the decline in quality that so many have observed.\n\n\n\n\n\n\n\n\n\nIn this plot, each episode of “The Simpsons” is represented by its TMBD rating, and episodes are colored by season. By visually inspecting the graph, we may already start to see some potential points where the ratings decline. However, the goal of our changepoint analysis is to move beyond visual inspection and rigorously detect the exact moment where a significant shift in the data occurs.\nJokes apart, this is a challenging time series! First of all, there’s not a clear single change, but rather an increase, followed by a decline. After which, the sequence seems rather stationary. For this reason, throughout the module, we will use this data as a running example to develop our understanding of various methods, hopefully trying to obtain a definitive answer towards the final chapters. But let’s proceed with order…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Changepoint Detection</span>"
    ]
  },
  {
    "objectID": "1_intro_cusum.html#detecting-one-change-in-mean-the-cusum-statistic",
    "href": "1_intro_cusum.html#detecting-one-change-in-mean-the-cusum-statistic",
    "title": "1  An Introduction to Changepoint Detection",
    "section": "1.3 Detecting one change-in-mean: the CUSUM Statistic",
    "text": "1.3 Detecting one change-in-mean: the CUSUM Statistic\n\n\n\n\nBown, Jonathan. 2023. “Simpsons Episodes & Ratings (1989-Present).” https://www.kaggle.com/datasets/jonbown/simpsons-episodes-2016?resource=download.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Changepoint Detection</span>"
    ]
  },
  {
    "objectID": "1_intro_cusum.html#detecting-one-change-in-mean",
    "href": "1_intro_cusum.html#detecting-one-change-in-mean",
    "title": "1  An Introduction to Changepoint Detection",
    "section": "1.3 Detecting one change in mean",
    "text": "1.3 Detecting one change in mean\nIn this section, we will start by exploring the simplest case of a changepoint detection problem: detecting a change in the mean of a time series. We assume that the data is generated according to the following model:\n\\[\ny_t = \\mu_t + \\epsilon_t, \\quad t = 1, \\dots, n,\n\\]\nwhere \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\) represents Gaussian noise with mean 0 and known variance \\(\\sigma^2\\), and \\(\\mu_t \\in \\mathbb{R}\\) is the signal at time \\(t\\). The vector of noise terms \\(\\epsilon_{1:n}\\) is often referred to as Gaussian noise, and hence, this model is known as the signal plus noise model, where the signal is given by \\(\\mu_{1:n}\\) and the noise by \\(\\epsilon_{1:n}\\).\nIn the change-in-mean problem, our goal is to determine whether the signal remains constant throughout the entire sequence, or if there exists a point \\(\\tau\\)), where the mean shifts. In other words, we are testing whether:\n\\[\n\\mu_1 = \\mu_2 = \\dots = \\mu_n \\quad \\text{(no changepoint)},\n\\]\nor if there exists a time \\(\\tau\\) such that:\n\\[\n\\mu_1 = \\mu_2 = \\dots = \\mu_\\tau \\neq \\mu_{\\tau+1} = \\dots = \\mu_n \\quad \\text{(changepoint at } \\tau\\text{)}.\n\\]\nNote. The point \\(\\tau\\) is our changepoint, e.g. the first point after which our mean changes, however there’s a lot of inconsistencies on the literature: sometimes you will find that people refer to \\(\\tau + 1\\) as the changepoint, and \\(\\tau\\) as the last pre-change point (as a matter of fact, please let me know if you spot this inconsistency anywhere in these notes!).\nTo address this problem, one of the most widely used methods is the CUSUM (Cumulative Sum) statistic. The basic idea behind the CUSUM statistic is to systematically compare the distribution of the data to the left and right of each possible changepoint \\(\\tau\\). By doing so, we can assess whether there is evidence of a significant change in the mean at a given point.\n\n1.3.1 The CUSUM statistics\nThe CUSUM statistic works by comparing, for each potential data point, the empirical mean (average) of the data to the left (before \\(\\tau\\)) with the empirical mean of the data to the right (after \\(\\tau\\)):\n\\[\nC_{\\tau} = \\sqrt{\\frac{\\tau(n-\\tau)}{n}} \\left| \\bar{y}_{1:\\tau} - \\bar{y}_{(\\tau+1):n} \\right|,\n\\]\nOur \\(\\bar{y}_{1:\\tau}\\) and \\(\\bar{y}_{(\\tau+1):n}\\) are just the empirical means of each segment, simply computed with:\n\\[\n\\bar{y}_{l:u} = \\frac{1}{u - l + 1} \\sum_{t = l}^{u} y_t.\n\\]\nThe term on the left of the difference, is there to re-scale it so that our statistics is the absolute value of normal RV that has variance 1. If there is no change, this difference is going to be distributed as a standard normal, and this is going to be a key step in drawing the distribution of the CUSUM statistic next week.\nThis approach is intuitive because if the mean \\(\\mu\\) is the same across the entire sequence, the values of the averages on both sides of any point \\(\\tau\\) should be similar. However, if there is a change in the mean, the means will differ significantly, highlighting the changepoint.\nMore formally, we would detect at change at \\(\\tau\\) if:\n\\[\n\\frac{C_{\\tau}^2 }{\\sigma^2} &gt; c,\n\\] where the \\(c \\in \\mathbb{R}^+\\) is a suitable chosen threshold value (similar to a critical value in hypothesis testing!).\n\n\n1.3.2 Algorithmic Formulation of the CUSUM Statistic\nIn practice, however, we do not know the changepoint location in advance. Our goal is to detect whether a changepoint exists and, if so, estimate its location. To achieve this, we need to consider all possible changepoint locations and choose the one that maximizes our test statistic.\nThe natural extension of the likelihood-ratio test to this situation is to use as a test statistic the maximum of \\(C_\\tau\\) as we vary \\(\\tau\\):\n\\[\nC_{max} = \\max_{\\tau \\in \\{1,\\ldots,n-1\\}} C_\\tau^2 / \\sigma^2\n\\]\nAnd detect a changepoint if \\(C_{max} &gt; c\\) for some suitably chosen threshold \\(c\\). The choice of \\(c\\) will determine the significance level of the test (we’ll discuss this in more detail later). Graphically, the test will look:\n\n\n\n\n\n\n\n\n\nIf we detect a changepoint (i.e., if \\(C_{max} &gt; c\\)), we can estimate its location by:\n\\[\n\\hat{\\tau} = \\arg\\max_{\\tau \\in \\{1,\\ldots,n-1\\}}  C_\\tau^2 / \\sigma^2\n\\]\nIn other words, \\(\\hat{\\tau}\\) is the value of \\(\\tau\\) that maximizes the CUSUM statistic.\nA simple estimate of the size of the change is then given by:\n\\[\n\\Delta\\hat{\\mu} = \\bar{y}_{(\\hat{\\tau}+1):n} - \\bar{y}_{1:\\hat{\\tau}}\n\\]\nThis estimate represents the difference between the mean of the data after the estimated changepoint and the mean of the data before the estimated changepoint.\nIn pseudocode:\n\nINPUT: Time series \\(y = (y_1, ..., y_n)\\), threshold \\(c\\), variance \\(\\sigma\\).\nOUTPUT: Changepoint estimate \\(\\hat{\\tau}\\), maximum CUSUM statistic \\(C_{max}\\)\n\n\\(n \\leftarrow\\) length of \\(y\\)\n\\(C_{max} \\leftarrow 0\\)\n\\(\\hat{\\tau} \\leftarrow 0\\)\n\\(S_n \\leftarrow \\sum_{i=1}^n y_i\\) // Compute total sum of y\n\\(S \\leftarrow 0\\)\n\nFOR \\(t = 1, \\dots, n - 1\\)\n  \\(S \\leftarrow S + y_t\\)\n  \\(\\bar{y}_{1:t} \\leftarrow S / t\\)\n  \\(\\bar{y}_{(t+1):n} \\leftarrow (S_n - S) / (n - t)\\)\n  \\(C_t \\leftarrow \\sqrt{\\frac{t(n-t)}{n}} |\\bar{y}_{1:t} - \\bar{y}_{(t+1):n}|\\)\n  IF \\(C_t &gt; C_{max}\\)\n    \\(C_{max} \\leftarrow C_t\\)\n    \\(\\hat{\\tau} \\leftarrow t\\)\n\nIF \\(C_{max}^2 / \\sigma &gt; c\\)\n  RETURN \\(\\hat{\\tau}\\), \\(C_{max}\\) // Changepoint detected\nELSE\n  RETURN NULL, \\(C_{max}\\) // No changepoint detected\n\nIt’s worth noting that, as we only need to calculate the CUSUM statistic for each potential changepoint once. By pre-computing the cumulative sum of all observations, this allows. The time complexity of this algorithm is \\(O(n)\\), where \\(n\\) is the length of the time series.\n\n\n1.3.3 Example\nIt is possible to see how the cusum behaves on this simple example below:\n\n\n\n\n\n\n\n\n\nRunning the CUSUM test, and maximising on our Simpsons episode, results in:\n\n\n\n\n\n\n\n\n\nThis results in episode Thirty Minutes over Tokyo being the last “good” Simpsons episode, with Beyond Blunderdome being the start of the decline, according to the Gaussian change-in-mean model!",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Changepoint Detection</span>"
    ]
  },
  {
    "objectID": "2_control.html#distribution-under-the-null-hypothesis",
    "href": "2_control.html#distribution-under-the-null-hypothesis",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "",
    "text": "2.1.1 The Chi-Squared Distribution\nThe chi-squared distribution is a continuous probability distribution of the sum of squares of k independent standard normal random variables. It’s commonly used in hypothesis testing and constructing confidence intervals. The shape of the distribution depends on its degrees of freedom (k). For k=1, it’s highly skewed, but as k increases, it becomes more symmetric and approaches a normal distribution.\nThe CUSUM statistic for a fixed τ, \\(C_τ^2/σ^2\\), follows a chi-squared distribution with 1 degree of freedom under the null hypothesis for several reasons:\n\nUnder the null hypothesis (no changepoint), the data comes from a single normal distribution with a constant mean.\nThe CUSUM statistic essentially compares two sample means: one before τ and one after τ.\nThe difference between these two sample means, when properly normalized, follows a standard normal distribution under the null hypothesis.\nWhen we square a standard normal random variable, the result follows a chi-squared distribution with 1 degree of freedom.\n\nThe “1” in “1 degree of freedom” comes from the fact that we’re essentially estimating one parameter: the difference between two means. Once this difference is known, there are no other free parameters in our comparison.\nTo visualize this, imagine a see-saw with the fulcrum at τ. The left side represents the data before τ, and the right side represents the data after τ. Under the null hypothesis, this see-saw should be balanced. The CUSUM statistic measures how much the see-saw tilts, and squaring it gives us a chi-squared distribution with 1 degree of freedom.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#towards",
    "href": "2_control.html#towards",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.2 Towards",
    "text": "2.2 Towards\nWhile the distribution for a fixed τ is straightforward, our actual test statistic for detecting a change is \\(\\max_τ C_τ^2/σ^2\\). Calculating the distribution of this maximum is challenging due to several factors:\n\nDependencies: The values of \\(C_τ\\) are not independent for different values of τ.\nGrowing dimensions: The number of potential changepoints (and thus, the number of \\(C_τ\\) values we’re maximizing over) grows with the sample size n.\nNon-standard conditions: The usual regularity conditions for likelihood-ratio test statistics don’t apply here. Setting the size of the actual change in mean to 0 effectively removes the changepoint parameter from the model.\n\nAs a result, even asymptotically (as n approaches infinity), the distribution of \\(\\max_τ C_τ^2/σ^2\\) under the null hypothesis is not chi-squared.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#asymptotic-distribution",
    "href": "2_control.html#asymptotic-distribution",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.3 2.4 Asymptotic Distribution",
    "text": "2.3 2.4 Asymptotic Distribution\nTo address these challenges, we turn to asymptotic theory. In time series analysis, an asymptotic distribution refers to the distribution that our test statistic approaches as the length of the time series (n) becomes very large. It’s a theoretical concept that helps us understand the behavior of our statistic for large samples.\nFor the CUSUM test, we can use the fact that \\((C_1, ..., C_{n-1})/σ\\) are the absolute values of a Gaussian process with mean 0 and known covariance. The maximum of a set of Gaussian random variables is known to converge to a Gumbel distribution (see Theorem 2.1 of Yao and Davis (1986) and Gombay and Horvath, 1990). This convergence is described by the following equation:\n\\(\\lim_{n→∞} Pr\\{a_n^{-1}(\\max_τ C_τ/σ - b_n) ≤ u\\} = \\exp\\{-2π^{-1/2}\\exp(-u)\\}\\),\nwhere \\(a_n = (2 \\log \\log n)^{-1/2}\\) and \\(b_n = a_n^{-1} + 0.5a_n \\log \\log \\log n\\).\n\n2.3.1 2.4.1 Understanding \\(a_n\\) and \\(b_n\\)\nThe quantities \\(a_n\\) and \\(b_n\\) in the asymptotic distribution formula are scaling and centering constants. They help adjust the maximum CUSUM statistic so that it converges to a stable distribution as n grows large.\n\n\\(a_n\\) is a scaling factor that decreases as n increases. It ensures that the maximum CUSUM statistic doesn’t grow too quickly with n.\n\\(b_n\\) is a centering constant that shifts the distribution. It grows slowly with n, compensating for the tendency of the maximum to increase as we consider more potential changepoints.\n\nTo understand why these specific forms are used, imagine you’re measuring the highest point in different countries. As you survey more countries (increasing n), you’re more likely to find higher peaks. The \\(a_n\\) and \\(b_n\\) terms adjust for this effect, allowing us to compare “record heights” fairly across different numbers of countries.\nIn our CUSUM context, these terms allow us to compare the significance of maximum CUSUM statistics across time series of different lengths. They ensure that our threshold for detecting a changepoint scales appropriately with the length of the series.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#practical-implications-and-considerations",
    "href": "2_control.html#practical-implications-and-considerations",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.2 2.5 Practical Implications and Considerations",
    "text": "2.2 2.5 Practical Implications and Considerations\nThis asymptotic result suggests that the threshold for \\(C_τ^2/σ^2\\) should increase with n at a rate of approximately \\(2 \\log \\log n\\). However, there are some practical considerations to keep in mind:\n\nSlow convergence: The convergence of \\(\\max_τ C_τ\\) to a Gumbel distribution is slow, meaning that for finite sample sizes, the asymptotic approximation may not be very accurate.\nConservative thresholds: The threshold suggested by this asymptotic distribution can be conservative in practice, potentially leading to fewer detected changepoints than actually exist.\nBetter approximations: Yao and Davis (1986) and Hawkins (1977) provide better approximations of the distribution of \\(\\max_τ C_τ^2/σ^2\\) for finite n, which can be more useful in practical applications.\nMonte Carlo methods: In practice, it’s often simplest and most effective to use Monte Carlo methods to approximate the null distribution of the test statistic. This approach involves simulating many time series under the null hypothesis (no changepoint) and calculating the test statistic for each. The distribution of these simulated test statistics can then be used to set appropriate thresholds.\n\nThe Monte Carlo approach has the additional advantage of being easily applicable to more complicated changepoint scenarios, making it a versatile tool in changepoint analysis.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#conclusion",
    "href": "2_control.html#conclusion",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.3 2.6 Conclusion",
    "text": "2.3 2.6 Conclusion\nUnderstanding the distribution of the CUSUM test statistic is crucial for setting appropriate thresholds and interpreting the results of changepoint detection analyses. While the asymptotic theory provides valuable insights into the behavior of the test statistic for large sample sizes, practical applications often require more nuanced approaches, such as finite-sample approximations or Monte Carlo methods.\nIn the next chapter, we’ll explore how to implement these methods in practice and discuss the power of the CUSUM test under various scenarios.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#the-asymptotic-distribution-of-the-cusum-statistics",
    "href": "2_control.html#the-asymptotic-distribution-of-the-cusum-statistics",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "",
    "text": "So far, we worked only for one fixed \\(\\tau\\), however, when comparing the maximums, the values of \\(C_\\tau\\) are in fact not independent across different \\(\\tau\\)s.\nAs we will learn later, the CUSUM is a special case of a LR test, as setting the size of the actual change in mean to 0 effectively removes the changepoint parameter from the model. For this reason, the usual regularity conditions for likelihood-ratio test statistics don’t apply here.\n\n\n2.1.1 Controlling the max of our cusums\nFortunately, for controlling our CUSUM test, we can use the fact that \\((C_1, ..., C_{n-1})/ \\sigma\\) are the absolute values of a Gaussian process with mean 0 and known covariance, and there are well known statistical results that can help us in our problem. Yao and Davis (1986), in fact, show that the maximum of a set of Gaussian random variables is known to converge to a Gumbel distribution, described by the following equation:\n\\[\n\\lim_{n→\\infty} \\text{Pr}\\{a_n^{-1}(\\max_τ C_τ/σ - b_n) ≤ u\\} = \\exp\\{-2π^{-1/2}\\exp(-u)\\},\n\\]\nwhere \\(a_n = (2 \\log \\log n)^{-1/2}\\) and \\(b_n = a_n^{-1} + 0.5a_n \\log \\log \\log n\\) are a scaling and a centering constant.\nThis asymptotic result suggests that the threshold for \\(C_τ^2/σ^2\\) should increase with \\(n\\) at a rate of approximately \\(2 \\log \\log n\\). Given that this is a fairly slow rate of convergence, this suggests that the threshold suggested by this asymptotic distribution can be conservative in practice, potentially leading to detect less changepoints than what actually exist.\nIn practice, it’s often simplest and most effective to use Monte Carlo methods to approximate the null distribution of the test statistic. This approach involves simulating many time series under the null hypothesis (no changepoint) and calculating the test statistic for each. The distribution of these simulated test statistics can then be used to set appropriate thresholds. This, as we will see, it’s much better as it ends up having less conservative thresholds:\n\nWe will see how to obtain this thresholds in the Lab.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "1_intro_cusum.html#workshop-1-exercises",
    "href": "1_intro_cusum.html#workshop-1-exercises",
    "title": "1  An Introduction to Changepoint Detection",
    "section": "1.4 Workshop 1 Exercises",
    "text": "1.4 Workshop 1 Exercises\n\nDetermine if the following processes are stationary, piecewise stationary, or non-stationary:\n\n\n\\(y_t = y_{t - 1} + \\epsilon_t, \\quad \\ t = 2, \\dots, n, y_1 = 0, \\epsilon_{1:n} \\sim N(0, 1)\\). Suggestion: try to compute the variance of the process.\n$ y_t = t + 3 (t &gt; 50),  t = 1, , 100, {1:100} N(0, 1) $\n\\(y_t = 0.05 \\cdot t + \\epsilon_t, \\ t = 1, \\dots, 100, \\quad \\epsilon_{1:100} \\sim N(0, 1)\\)\n\n\nCompute the mean and variance of the absolute value of difference between the two pre-change and post-change sample means when there is no change, and show that when multiplied by the normalizing constant \\(\\frac{1}{\\sigma^2} \\sqrt{\\frac{\\tau(n-\\tau)}{n}}\\) this follows a standard normal distribution.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Changepoint Detection</span>"
    ]
  },
  {
    "objectID": "1_intro_cusum.html#lab-1-exercises",
    "href": "1_intro_cusum.html#lab-1-exercises",
    "title": "1  An Introduction to Changepoint Detection",
    "section": "1.5 Lab 1 Exercises",
    "text": "1.5 Lab 1 Exercises\n\nCode the CUSUM algorithm for an unknown change location, based on the pseudocode above.\nCompute the CUSUM statistics for all changepoint locations in the Simpsons data, plot the result over time. Check it against the plot above.\n\n\n\n\n\nBown, Jonathan. 2023. “Simpsons Episodes & Ratings (1989-Present).” https://www.kaggle.com/datasets/jonbown/simpsons-episodes-2016?resource=download.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Changepoint Detection</span>"
    ]
  },
  {
    "objectID": "2_control.html#workshop-2-exercises",
    "href": "2_control.html#workshop-2-exercises",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.4 Workshop 2 Exercises",
    "text": "2.4 Workshop 2 Exercises\n\nGiven the asymptotic result on the distribution, for \\(n = 100\\) and a false positive rate \\(\\alpha = 0.05\\), calculate the threshold \\(c\\).\nShow that the likelihood ratio test statistic for a change in mean in a Gaussian model is the CUSUM statistics squared, rescaled by the known variance, e.g. \\(LR_\\tau = C_\\tau^2/\\sigma^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#lab-2-exercises",
    "href": "2_control.html#lab-2-exercises",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.5 Lab 2 Exercises",
    "text": "2.5 Lab 2 Exercises\n\nWrite a function, that taking as input \\(n\\) and a desired \\(\\alpha\\) level for false positive rate, returns the treshold for the cusum statistics.\nConstruct a function that, taking as input \\(n\\), a desired \\(\\alpha\\) , and a replicates parameter, runs a Monte Carlo simulation to tune an empirical penalty for the CUSUM change-in-mean on a simple Gaussian signal. Tip: You can reuse the function for computing the CUSUM statistics that you built the last week\nCompare for a range of increasingly values of n, e.g. \\(n = 100, 500, 1000, 10.000\\), and for few desired levels of alpha, the Monte Carlo threshold with the theoretically justified threshold. Plot the results, to recreate the plot above.\nUsing the Test the Simpsons dataset, find a critical level for your CUSUM statistics, and declare a change with the change-in-mean model.\n\n\n\n\n\nBaranowski, Rafal, Yining Chen, and Piotr Fryzlewicz. 2019. “Narrowest-over-Threshold Detection of Multiple Change Points and Change-Point-Like Features.” Journal of the Royal Statistical Society Series B: Statistical Methodology 81 (3): 649–72.\n\n\nYao, Yi-Ching, and Richard A Davis. 1986. “The Asymptotic Behavior of the Likelihood Ratio Statistic for Testing a Shift in Mean in a Sequence of Independent Normal Variates.” Sankhyā: The Indian Journal of Statistics, Series A, 339–53.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#towards-more-general-models-the-likelihood-ratio-test",
    "href": "2_control.html#towards-more-general-models-the-likelihood-ratio-test",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.2 Towards More General Models: the Likelihood Ratio Test",
    "text": "2.2 Towards More General Models: the Likelihood Ratio Test\nThe CUSUM can be viewed as a special case of a more general framework based on the Likelihood Ratio Test (LRT). This allow us to test for more general settings, beyond simply detecting changes in the mean.\nIn general, the Likelihood Ratio Test is a method for comparing two nested models: one under the null hypothesis, which assumes no changepoint, and one under the alternative hypothesis, which assumes a changepoint exists at some unknown position \\(\\tau\\).\nSuppose we have a set of observations \\(x_1, x_2, \\dots, x_n\\). Under the null hypothesis \\(H_0\\), we assume that all the data is generated by the same model without a changepoint. Under the alternative hypothesis \\(H_1\\), there is a single changepoint at \\(\\tau\\), such that the model for the data changes after \\(\\tau\\). The LRT statistic is given by:\n\\[\nLR_\\tau = - 2 \\log \\left\\{ \\frac{\\max_{\\theta} \\prod_{t=1}^n L(y_{t}| \\theta)}{\\max_{\\theta_1, \\theta_2} [(\\prod_{t=1}^n L(y_{t}| \\theta_1))(\\prod_{t=1}^n L(y_{t}| \\theta_2)]} \\right\\}\n\\]\nThe LRT compares the likelihood of the data under two models to determine which one is more likely: the enumerator, is the likelihood under the null hypothesis of no changepoint, while the denominator represents the likelihood of the data under the alternative hypothesis, where we optimise for two different parameters before and after the changepoint at \\(\\tau\\).\n\n2.2.1 Example: Gaussian change-in-mean\nThe CUSUM statistics, in fact, is nothing but a specific case of this model. To see this, we start from our piecewise costant signal, plus noise, \\(x_i = f_i + \\epsilon_i, \\quad i = 1, \\dots, n\\). Under this model our data, a linear combination of a Gaussian, is distributed as:\n\\[\ny_{i} \\sim N(\\mu_i, \\sigma^2), \\quad i = 1, \\dots, n\n\\] Therefore, to obtain the likelihood ratio test statistic, we plug our Gaussian kernel into the LR above, and take the logarithm:\n\\[\nLR_\\tau = -2 \\left[ \\max_{\\mu} \\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mu)^2 \\right) - \\max_{\\mu_1, \\mu_2} \\left( -\\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^\\tau (y_i - \\mu_1)^2 + \\sum_{i=\\tau+1}^n (y_i - \\mu_2)^2 \\right) \\right)  \\right]\n\\]\nThis simplifies to:\n\\[\n= \\frac{1}{\\sigma^2} \\left[ \\min_{\\mu} \\sum_{i=1}^n (y_i - \\mu_1)^2 - \\min_{\\mu_1, \\mu_2} \\left( \\sum_{i=1}^\\tau (y_i - \\mu_1)^2 + \\sum_{i=\\tau+1}^n (y_i - \\mu_2)^2 \\right) \\right]\n\\]\nTo solve the minimization over \\(\\mu_1\\) and \\(\\mu_2\\), we plug-in values \\(\\hat\\mu = \\bar{y}_{1:n}\\) on the first term, and \\(\\hat\\mu_1 = \\bar{y}_{1:\\tau}\\), \\(\\hat\\mu_2 = \\bar{y}_{(\\tau+1):n}\\) for the second term:\n\\[\nLR_\\tau = \\frac{1}{\\sigma^2} \\left[ \\sum_{i=1}^n (y_i - \\bar{y}_{1:n})^2 - \\sum_{i=1}^\\tau (y_i - \\bar{y}_{1:\\tau})^2 - \\sum_{i=\\tau+1}^n (y_i - \\bar{y}_{(\\tau+1):n})^2 \\right]\n\\]\nThis is the likelihood ratio test statistic for a change in mean in a Gaussian model, which is essentially the CUSUM statistics squared, rescaled by the known variance:\n\\[\nLR_\\tau = \\frac{C_\\tau^2}{\\sigma^2}\n\\]\nThis is possible to prove directly after some tedious algebraic manipulations (which we will see in the workshop!).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#change-in-variance",
    "href": "2_control.html#change-in-variance",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.3 Change-in-variance",
    "text": "2.3 Change-in-variance\nThe great thing of the LR test is that it’s extremely flexible, allowing us to detect other changes then the simple change-in-mean case. As before, the procedure is to compute the LR test conditional on a fixed location of a changepoint, e.g. \\(LR_\\tau\\), and range across all possible values for \\(\\tau\\) to find the test statistics for our change.\nTo this end we will demostrate how to construct a test for Gaussian change-in-variance, for mean known. For simplicy, we will call our variance \\(\\sigma^2 = \\theta\\), our parameter of interest, and without loss of generality, we can center our data on zero (e.g. if \\(x_t \\sim N(\\mu, \\theta)\\), then \\(x_t - \\mu = y_t \\sim N(0, \\theta)\\)). Then, our p.d.f for one observation will be given by:\n\\[\nL(y_t | \\theta) = \\frac{1}{\\theta \\sqrt{2\\pi}} \\exp\\{-\\frac{y_t}{2 \\theta}\\}.\n\\] Plugging in the main LR test formula, we find:\n\\[\nLR_\\tau = - 2 \\log \\left\\{ \\frac{\\max_{\\theta} \\prod_{t=1}^n \\frac{1}{\\theta \\sqrt{2\\pi}} \\exp\\{-\\frac{y_t}{2 \\theta}\\}}{\\max_{\\theta_1, \\theta_2} [(\\prod_{t=1}^\\tau \\frac{1}{\\theta \\sqrt{2\\pi}} \\exp\\{-\\frac{y_t}{2 \\theta}\\})(\\prod_{t=\\tau+1}^n  \\frac{1}{\\theta \\sqrt{2\\pi}} \\exp\\{-\\frac{y_t}{2 \\theta}\\}]} \\right\\}\n\\]\nAnd taking the log, and simplifying over the constant gives us:\n\\[\n\\min_\\theta \\sum_{t = 1}^n \\left(  - \\log(\\theta) - \\frac{y^2}{\\theta} \\right) - \\min_{\\theta_1, \\theta_2}  \\left[ \\ \\sum_{t = 1}^\\tau \\left(  - \\log(\\theta_1) - \\frac{y^2}{\\theta_1} \\right) + \\sum_{t = \\tau+1}^n \\left(  - \\log(\\theta_2) - \\frac{y^2}{\\theta_2} \\right) \\right]\n\\]\nNow to solve the minimisation, we focus on the first term: \\[\nf(y_{1:n}, \\theta) = \\sum_{t = 1}^n \\left(  - \\log(\\theta) - \\frac{y^2}{\\theta} \\right)\n\\] Simplifying the sum: \\[\n= \\left( - n \\log(\\theta) - \\frac{\\sum_{t = 1}^n y^2}{\\theta} \\right).\n\\]\nTaking the derivative with respect to \\(\\theta\\), gives:\n\\[\n\\frac{d}{d\\theta} f(y_{1:n}, \\theta) = -\\frac{n}{\\theta} + \\frac{\\sum_{t = 1}^n y^2}{\\theta^2}\n\\] Setting equal to zero and solving for \\(\\theta\\): \\[\n-n \\theta + \\sum_{t = 1}^n y^2 = 0\n\\] Which gives us: \\(\\hat\\theta = \\frac{\\sum_{t = 1}^n y^2}{n} = \\bar S_{1:n}\\) the sample variance.\nSolving the optimization for \\(\\theta_1\\) and \\(\\theta_2\\) similarly, and plugging in the values \\(\\hat \\theta_1 = \\bar S_{1:\\tau}, \\ \\hat \\theta_2 =  \\bar S_{(\\tau+1):n}\\), gives us the final LR test:\n\\[\nLR_\\tau = \\left[  - n \\log(\\bar S_{1:n}) + \\tau \\log(\\bar S_{1:\\tau}) + (n - \\tau) \\log(\\bar S_{(\\tau + 1):n}) \\right].\n\\]\n\nLR_var &lt;- function(x) {\n  x2 &lt;- x^2 # squaring the data\n  S &lt;- cumsum(x2) # calculate square cummulate sum of the data, divided by n\n  n &lt;- length(x) #number of data points\n  tau &lt;- 1:(n - 1) #possible change-point locations to test\n  left &lt;- - tau * log(S[tau] / tau)\n  right &lt;- - (n - tau) * log((S[n] - S[tau]) / (n-tau))\n  LR &lt;- - n * log(S[n]/n) + left + right #LR statistic at locations tau\n  LR &lt;- LR\n  #return LR statistic and estimate of tau\n  return(list(LR = LR, LR_max = max(LR), tau.hat = which.max(LR)))\n}\n\ny &lt;- c(rnorm(100), rnorm(100, 0, 1))\n\nLR_var(y)\n\n$LR\n  [1] 32.40970 33.22922 34.38975 35.46104 34.19831 35.32917 35.07842 32.78068\n  [9] 33.13448 33.06834 33.32437 33.57142 32.64705 32.94830 33.18676 33.55366\n [17] 34.02525 34.25268 34.25816 34.05217 34.54485 34.43246 34.78147 35.28590\n [25] 33.63885 33.81470 34.19491 34.64143 34.54554 35.01553 34.25032 34.56268\n [33] 35.01532 35.38727 33.63057 33.46245 33.05552 33.14119 32.53421 32.38446\n [41] 32.42772 32.53554 32.35394 32.27393 32.26739 32.25901 32.28555 32.27008\n [49] 32.54401 32.64189 32.59443 32.54682 32.44993 32.45771 32.37181 32.31377\n [57] 32.28490 32.26347 32.26394 32.25941 32.25893 32.25937 32.26858 33.65035\n [65] 33.45411 33.25453 33.13813 32.99653 32.87350 32.74167 32.61407 32.50160\n [73] 32.41037 32.87648 32.81145 32.85535 32.70797 32.59358 32.50627 32.43406\n [81] 32.45299 32.52237 32.43980 32.42361 32.35555 32.30479 32.32023 32.28073\n [89] 32.27080 32.26020 32.26251 32.28107 32.32050 32.32710 32.28934 32.28402\n [97] 32.26843 32.28688 32.27052 32.28453 32.60096 32.71089 32.59513 32.52090\n[105] 32.44414 32.38484 32.36284 32.32683 32.46881 32.38698 32.32587 32.28598\n[113] 32.31190 32.28402 32.28633 32.39895 32.34547 32.32163 32.36028 32.41932\n[121] 32.37112 32.33462 32.35541 32.32856 32.28971 32.26416 32.25953 32.25939\n[129] 32.26033 32.25956 32.35351 32.30007 32.26938 32.26241 32.25902 32.26616\n[137] 32.25927 32.27013 32.29962 32.31607 32.33474 32.38512 32.29959 32.28403\n[145] 32.32962 32.26248 32.26165 32.26278 32.25894 32.26584 32.29175 32.32553\n[153] 32.38862 32.48594 32.33430 32.41329 32.39186 32.44803 32.56704 32.39934\n[161] 32.47376 32.56953 32.72553 32.60480 32.25895 32.30739 32.26672 32.43502\n[169] 32.35156 32.28630 32.27718 32.27791 32.26339 32.31499 32.30176 32.26655\n[177] 32.26289 32.30703 32.38019 32.51900 32.29824 32.33280 32.43454 32.59967\n[185] 32.83947 33.14835 33.51980 32.94874 33.30865 33.76703 33.66775 33.69051\n[193] 33.62485 34.09019 32.73223 33.25199 32.32574 32.28528 32.42684\n\n$LR_max\n[1] 35.46104\n\n$tau.hat\n[1] 4",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#the-likelihood-ratio-test",
    "href": "2_control.html#the-likelihood-ratio-test",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.2 The Likelihood Ratio Test",
    "text": "2.2 The Likelihood Ratio Test\nThe CUSUM can be viewed as a special case of a more general framework based on the Likelihood Ratio Test (LRT). This allow us to test for more general settings, beyond simply detecting changes in the mean.\nIn general, the Likelihood Ratio Test is a method for comparing two nested models: one under the null hypothesis, which assumes no changepoint, and one under the alternative hypothesis, which assumes a changepoint exists at some unknown position \\(\\tau\\).\nSuppose we have a set of observations \\(x_1, x_2, \\dots, x_n\\). Under the null hypothesis \\(H_0\\), we assume that all the data is generated by the same model without a changepoint. Under the alternative hypothesis \\(H_1\\), there is a single changepoint at \\(\\tau\\), such that the model for the data changes after \\(\\tau\\). The LRT statistic is given by:\n\\[\nLR_\\tau = - 2 \\log \\left\\{ \\frac{\\max_{\\theta} \\prod_{t=1}^n L(y_{t}| \\theta)}{\\max_{\\theta_1, \\theta_2} [(\\prod_{t=1}^n L(y_{t}| \\theta_1))(\\prod_{t=1}^n L(y_{t}| \\theta_2)]} \\right\\}\n\\]{eq-lr-test}\nThe LRT compares the likelihood of the data under two models to determine which one is more likely: the enumerator, is the likelihood under the null hypothesis of no changepoint, while the denominator represents the likelihood of the data under the alternative hypothesis, where we optimise for two different parameters before and after the changepoint at \\(\\tau\\).\n\n2.2.1 Example: Gaussian change-in-mean\nThe CUSUM statistics, in fact, is nothing but a specific case of this model. To see this, we start from our piecewise costant signal, plus noise, \\(x_i = f_i + \\epsilon_i, \\quad i = 1, \\dots, n\\). Under this model our data, a linear combination of a Gaussian, is distributed as:\n\\[\ny_{i} \\sim N(\\mu_i, \\sigma^2), \\quad i = 1, \\dots, n\n\\] Therefore, to obtain the likelihood ratio test statistic, we plug our Gaussian kernel into the LR above, and take the logarithm:\n\\[\nLR_\\tau = -2 \\left[ \\max_{\\mu} \\left( -\\frac{1}{2\\sigma^2} \\sum_{i=1}^n (y_i - \\mu)^2 \\right) - \\max_{\\mu_1, \\mu_2} \\left( -\\frac{1}{2\\sigma^2} \\left( \\sum_{i=1}^\\tau (y_i - \\mu_1)^2 + \\sum_{i=\\tau+1}^n (y_i - \\mu_2)^2 \\right) \\right)  \\right]\n\\]\nThis simplifies to:\n\\[\n= \\frac{1}{\\sigma^2} \\left[ \\min_{\\mu} \\sum_{i=1}^n (y_i - \\mu_1)^2 - \\min_{\\mu_1, \\mu_2} \\left( \\sum_{i=1}^\\tau (y_i - \\mu_1)^2 + \\sum_{i=\\tau+1}^n (y_i - \\mu_2)^2 \\right) \\right]\n\\]\nTo solve the minimization over \\(\\mu_1\\) and \\(\\mu_2\\), we plug-in values \\(\\hat\\mu = \\bar{y}_{1:n}\\) on the first term, and \\(\\hat\\mu_1 = \\bar{y}_{1:\\tau}\\), \\(\\hat\\mu_2 = \\bar{y}_{(\\tau+1):n}\\) for the second term:\n\\[\nLR_\\tau = \\frac{1}{\\sigma^2} \\left[ \\sum_{i=1}^n (y_i - \\bar{y}_{1:n})^2 - \\sum_{i=1}^\\tau (y_i - \\bar{y}_{1:\\tau})^2 - \\sum_{i=\\tau+1}^n (y_i - \\bar{y}_{(\\tau+1):n})^2 \\right]\n\\]\nThis is the likelihood ratio test statistic for a change in mean in a Gaussian model, which is essentially the CUSUM statistics squared, rescaled by the known variance:\n\\[\nLR_\\tau = \\frac{C_\\tau^2}{\\sigma^2}\n\\]\nThis is possible to prove directly after some tedious algebraic manipulations (which we will see in the workshop!).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#towards-more-general-models",
    "href": "2_control.html#towards-more-general-models",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.3 Towards More General Models",
    "text": "2.3 Towards More General Models\nThe great thing of the LR test is that it’s extremely flexible, allowing us to detect other changes then the simple change-in-mean case. As before, the procedure is to compute the LR test conditional on a fixed location of a changepoint, e.g. \\(LR_\\tau\\), and range across all possible values for \\(\\tau\\) to find the test statistics for our change.\n\n2.3.1 Change-in-variance\nTo this end we will demostrate how to construct a test for Gaussian change-in-variance, for mean known. For simplicy, we will call our variance \\(\\sigma^2 = \\theta\\), our parameter of interest, and without loss of generality, we can center our data on zero (e.g. if \\(x_t \\sim N(\\mu, \\theta)\\), then \\(x_t - \\mu = y_t \\sim N(0, \\theta)\\)). Then, our p.d.f for one observation will be given by:\n\\[\nL(y_t | \\theta) = \\frac{1}{\\theta \\sqrt{2\\pi}} \\exp\\{-\\frac{y_t}{2 \\theta}\\}.\n\\] Plugging in the main LR test formula, we find:\n\\[\nLR_\\tau = - 2 \\log \\left\\{ \\frac{\\max_{\\theta} \\prod_{t=1}^n \\frac{1}{\\theta \\sqrt{2\\pi}} \\exp\\{-\\frac{y_t}{2 \\theta}\\}}{\\max_{\\theta_1, \\theta_2} [(\\prod_{t=1}^\\tau \\frac{1}{\\theta \\sqrt{2\\pi}} \\exp\\{-\\frac{y_t}{2 \\theta}\\})(\\prod_{t=\\tau+1}^n  \\frac{1}{\\theta \\sqrt{2\\pi}} \\exp\\{-\\frac{y_t}{2 \\theta}\\}]} \\right\\}\n\\]\nAnd taking the log, and simplifying over the constant gives us:\n\\[\n\\min_\\theta \\sum_{t = 1}^n \\left( \\log(\\theta) + \\frac{y^2}{\\theta} \\right) - \\min_{\\theta_1, \\theta_2}  \\left[ \\ \\sum_{t = 1}^\\tau \\left(  \\log(\\theta_1) + \\frac{y^2}{\\theta_1} \\right) + \\sum_{t = \\tau+1}^n \\left(   \\log(\\theta_2) + \\frac{y^2}{\\theta_2} \\right) \\right]\n\\]\nNow to solve the minimisation, we focus on the first term: \\[\nf(y_{1:n}, \\theta) = \\sum_{t = 1}^n \\left(  - \\log(\\theta) - \\frac{y^2}{\\theta} \\right)\n\\] Simplifying the sum: \\[\n= \\left( - n \\log(\\theta) - \\frac{\\sum_{t = 1}^n y^2}{\\theta} \\right).\n\\]\nTaking the derivative with respect to \\(\\theta\\), gives:\n\\[\n\\frac{d}{d\\theta} f(y_{1:n}, \\theta) = -\\frac{n}{\\theta} + \\frac{\\sum_{t = 1}^n y^2}{\\theta^2}\n\\] Setting equal to zero and solving for \\(\\theta\\): \\[\n-n \\theta + \\sum_{t = 1}^n y^2 = 0\n\\] Which gives us: \\(\\hat\\theta = \\frac{\\sum_{t = 1}^n y^2}{n} = \\bar S_{1:n}\\) the sample variance.\nSolving the optimization for \\(\\theta_1\\) and \\(\\theta_2\\) similarly, and plugging in the values \\(\\hat \\theta_1 = \\bar S_{1:\\tau}, \\ \\hat \\theta_2 = \\bar S_{(\\tau+1):n}\\), gives us the final LR test:\n\\[\nLR_\\tau = \\left[  - n \\log(\\bar S_{1:n}) + \\tau \\log(\\bar S_{1:\\tau}) + (n - \\tau) \\log(\\bar S_{(\\tau + 1):n}) \\right].\n\\]\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Change-in-slope\nAnother important exaple, and an alternative to detecting a change-in-mean, is detecting a change in slope. In this section, we assume the data is still modeled as a signal plus noise, but the signal itself is a linear function of time (e.g. non-stationary, with a change!). Graphically:\n\n\n\n\n\n\n\n\n\nMore formally, let our data be modeled as:\n\\[\ny_t = f_t + \\epsilon_t, \\quad t = 1, \\dots, n,\n\\]\nwhere the noise vector \\(\\epsilon_{1:n} \\sim N(0, 1)\\) consists of independent and identically distributed (i.i.d.) normal random variables. In this scenario, for simplicity, we assume a known constant variance, which without loss of generality, we take to be 1.\nUnder the null hypothesis \\(H_0\\), we assume that the signal is linear with a constant slope over the entire sequence, i.e.,\n\\[\nf_t = \\theta_0 + t\\theta_1, \\quad t = 1, \\dots, n,\n\\]\nwhere \\(\\theta_0\\) is the intercept, and \\(\\theta_1\\) is the slope. However, under the alternative hypothesis \\(H_1\\), we assume there is a changepoint at \\(\\tau\\) after which the slope changes. Thus, the signal becomes:\n\\[\nf_t = \\theta_0 + t\\theta_1, \\quad t = 1, \\dots, \\tau; \\quad f_t = \\theta_0 + \\tau \\theta_1 + (t-\\tau)\\theta_2, \\quad t = \\tau+1, \\dots, n,\n\\]\nwhere \\(\\theta_2\\) is the new slope after the changepoint. In other words, the model is showing a continuoos piecewise linear mean.\nFor this model, the log-likelihood ratio test statistic can be written as the square of a projection of the data onto a vector \\(v_\\tau\\), i.e.,\n\\[\nLR_\\tau = \\left( v_\\tau^\\top y_{1:n} \\right)^2,\n\\]\nwhere \\(v_\\tau\\) is a contrast vector that is piecewise linear with a change in slope at \\(\\tau\\). This vector is constructed such that, under the null hypothesis, the vector \\(v_\\tau^\\top y_{1:n}\\) has variance 1, and $ v_^y_{1:n}$ is invariant to adding a linear function to the data. These properties uniquely define the contrast vector \\(v_\\tau\\), up to an arbitrary sign. Computations on how to obtain this likelihood ration test, and how to construct this vector are beyond the scope of this module, but should you be curious those are detailed in Baranowski, Chen, and Fryzlewicz (2019).\n\n\n2.3.3 Revisiting our Simpsons data (again!)\nSo, going back to the Simpsons example… We mentioned how the belowed show rose rapidly to success, and at one point, started to decline… A much better model would therefore be our change-in-slope model!\nTo run the model, we can take advantage of the not package, which by default is a multiple changepoint algorithm (we will see these in the next week), but whose simplest case implements exactly our change-in-slope LR test.\nBefore we proceed, we need to load, clean and standardize our data:\n\n# Load Simpsons ratings data\nsimpsons_episodes &lt;- read.csv(\"extra/simpsons_episodes.csv\")\nsimpsons_episodes &lt;- simpsons_episodes |&gt; \n  mutate(Episode = id + 1, Season = as.factor(season), Rating = tmdb_rating)\nsimpsons_episodes &lt;- simpsons_episodes[-nrow(simpsons_episodes), ]\n\ny &lt;- simpsons_episodes$Rating\n\nWe can then run our model with:\n\nlibrary(not)\n\nr &lt;- not(y, method = \"max\", rand.intervals = FALSE, contrast = \"pcwsLinContMean\", intervals = random.intervals(n, 1, min.length = (n-1)))\n\nprint(paste0(\"Our changepoint estimate: \",  features(r)$cpt))\n\n[1] \"Our changepoint estimate: NA\"\n\nplot(r)\n\n\n\n\n\n\n\n\nWe notice how the test statistics implemented does not detect any significant change… As this may sound a bit anti-climatic, there is still some relaxation on the hypotheses that we can do to obtain a better answer. We can, in fact, relax the hypothesis that the regression line needs to be continuous, and assume that there might be a change in the intercept too, signifying an abrupt change:\n\\[\nf_t = \\theta_0 + t\\theta_1, \\quad t = 1, \\dots, \\tau; \\quad f_t = \\theta_0 + \\theta_2 + \\tau \\theta_1 + (t-\\tau)\\theta_3, \\quad t = \\tau+1, \\dots, n,\n\\]\nWe can do that by changing the model via the contrast argument:\n\nr &lt;- not(y, method = \"max\", rand.intervals = FALSE, contrast = \"pcwsLinMean\", intervals = random.intervals(n, 1, min.length = (n-1)))\n\nprint(paste0(\"Our changepoint estimate: \",  features(r)$cpt))\n\n[1] \"Our changepoint estimate: 176\"\n\nplot(r)\n\n\n\n\n\n\n\n\nWe can see that we now find a significant changepoint prior to episode The Simpsons Spin-Off Showcase, which is anthology episode well over into season 8, which is by many considered the last good one!",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "3_multiple_changes.html",
    "href": "3_multiple_changes.html",
    "title": "3  Multiple changepoints",
    "section": "",
    "text": "3.1 Introduction\nIn real-world data, it is common to encounter situations where more than one change occurs. When applying the CUSUM statistic in such cases, where there are multiple changes, the question arises: how does CUSUM behave, and how can we detect these multiple changes effectively?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple changepoints</span>"
    ]
  },
  {
    "objectID": "3_multiple_changes.html#binary-segmentation",
    "href": "3_multiple_changes.html#binary-segmentation",
    "title": "3  Multiple changepoints",
    "section": "3.2 Binary Segmentation",
    "text": "3.2 Binary Segmentation\nBinary Segmentation (BS) is a procedure from and . Binary segmentation works like this:\n\nStart with a test for a change \\(\\tau\\) that splits a sequence into two segments and to check if the cost over those two segments, plus a penalty \\(\\beta \\in \\mathbb{R}\\), is smaller then the cost computed on the whole sequence: \\[\n    \\mathcal{L}(y_{1:\\tau}) + \\mathcal{L}(y_{\\tau+1:n}) + \\beta &lt; \\mathcal{L}(y_{1:n})     \n\\tag{3.2}\\]\n\nwhere the segment cost \\(\\mathcal{L}(\\cdot)\\), is as in Equation 3.1.\n\nIf the condition in Equation 3.2 is true for at least one \\(\\tau \\in 1, \\dots, n\\), then the \\(\\tau\\) that minimizes \\(\\mathcal{L}(y_{1:\\tau}) + \\mathcal{L}(y_{\\tau+1:n})\\) is picked as a first changepoint and the test is then performed on the two newly generated splits. This step is repeated until no further changepoints are detected on all resulting segments.\nIf there are no more resulting valid splits, then the procedure ends.\n\nSome of you might have noted how the condition in Equation 3.2 is closely related to the LR test in ?eq-lr-test. In fact, rearranging equation above, gives us:\n\\[\n- \\mathcal{L}(y_{1:n}) + \\mathcal{L}(y_{1:\\tau}) + \\mathcal{L}(y_{\\tau+1:n}) = - \\frac{LR_\\tau}{2}  &lt; -\\beta.\n\\]\nThe \\(-\\beta\\) acts exactly as the constant \\(c\\) for declaring a change, and it adds a natural stopping condition, solving the issue of overfitting that we mentioned in the previous section! Binary Segmentation, in fact, does nothing more then iteratively running a LR test, until no changes are found anymore!\nThis gives us a strategy to essentially apply a test that is locally optimal for one change, such as the Likelihood Ratio test, to solve a multiple changepoint segmentation. For this reason, BS is often employed to extend single changepoint procedures to multiple changes procedures, and hence it is one of the most prominent methods in the literature.\n\n3.2.1 Binary Segmentation in action\nHaving introduced the main idea, we show now how binary segmentation works in action with an example above. Say that we set a \\(\\beta = 2 \\log(400) =\\) 11.98.\nStep 1: We start by computing the cost as in Equation 3.2, and for those that are less then \\(\\beta\\), we pick the smallest. This will be our first changepoint estimate, and the first point of split.\nIn the plots below, the blue horizontal line is the mean signal estimated for a given split, while in the cusum the pink will represent the values of the LR below the threshold \\(\\beta\\), and red vertical line will show the min of the test statistics. When the cost is below the beta line, this will be our changepoint estimate.\nIn our case, we can see that the min of our cost has been achieved for \\(\\hat\\tau=100\\), and since this is below the threshold, it’s our first estimated changepoint!\n\n\n\n\n\n\n\n\n\nStep 2:\nFrom the first step, we have to check now two splits:\n\nThe first left split, 1-LEFT in the plot below, covers data \\(y_{1:100}\\). We can see that from here, the min of our statistic is below the threshold, therefore we won’t declare any further change in this subset.\nThe first right split, 1-RIGHT covers data \\(y_{101:400}\\). We can see that here, the min of the statistics, is below the threshold, and therefore we identify a second change at \\(\\hat\\tau = 297\\). This is not exactly 300, so we don’t have a perfect estimate. Despite this is not ideal, this is the best point we have found and therefore we have to continue!\n\n\n\n\n\n\n\n\n\n\nStep 3:\nIn step 3, we have to check again two splits splits:\n\nThe second left split, 2-LEFT in the plot below, covers data \\(y_{101:297}\\). Now, it’s in this split that the statistics goes below the threshold! The third estimated change is at \\(\\hat\\tau = 203\\), again slightly off the real one at 200. We continue investigating this split…\nThe second right split, 2-RIGHT covers data \\(y_{298:400}\\). In this last split, the min is not over the threshold, therefore we stop the search.\n\n\n\n\n\n\n\n\n\n\nStep 4:\nIn step 4, we check:\n\nThe third left split, 3-LEFT in the plot below, covers data \\(y_{101:203}\\). The minimum, in here is not over the threshold.\nThe third right split, 3-RIGHT covers data \\(y_{204:298}\\). Similarly, the minimum is not over the treshold.\n\n\n\n\n\n\n\n\n\n\nThe algorithm therefore terminates!\nWith this graphical description in mind, we formally describe the Binary Segmentation algorithm as a recursive procedure, where the first iteration would be simply given by \\(\\text{BinSeg}(y_{1:n}, \\beta)\\).\n\n\\(\\text{BinSeg}(y_{s:t}, \\beta)\\)\n\n\nINPUT: Subseries \\(y_{s:t} = \\{y_s, \\dots, y_t\\}\\) of length \\(t - s + 1\\), penalty \\(\\beta\\)\nOUTPUT: Set of detected changepoints \\(cp\\)\n\nIF \\(t - s \\leq 1\\)\n    RETURN \\(\\{\\}\\) // No changepoint in segments of length 1 or less\n\nCOMPUTE\n\\(\\mathcal{Q} \\leftarrow \\underset{\\tau \\in \\{s, \\dots, t\\}}{\\min} \\left[ \\mathcal{L}(y_{s:\\tau}) + \\mathcal{L}(y_{\\tau+1:t}) - \\mathcal{L}(y_{s:t}) + \\beta \\right]\\)\n\nIF \\(\\mathcal{Q} &lt; 0\\)\n    \\(\\hat{\\tau} \\leftarrow \\underset{\\tau \\in \\{s, \\dots, t\\}}{\\text{arg}\\min} \\left[ \\mathcal{L}(y_{s:\\tau}) + \\mathcal{L}(y_{\\tau+1:t}) - \\mathcal{L}(y_{s:t}) \\right]\\)\n     \\(cp \\leftarrow \\{ \\hat{\\tau}, \\text{BinSeg}(y_{s:\\hat{\\tau}}, \\beta), \\text{BinSeg}(y_{\\hat{\\tau}+1:t}, \\beta) + \\hat\\tau \\}\\)\n     RETURN \\(cp\\)\n\nRETURN \\(\\{\\}\\) // No changepoint if \\(-LR/2\\) is above penalty \\(- \\beta\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple changepoints</span>"
    ]
  },
  {
    "objectID": "3_multiple_changes.html#optimal-segmentation",
    "href": "3_multiple_changes.html#optimal-segmentation",
    "title": "3  Multiple changepoints",
    "section": "3.3 Optimal Segmentation",
    "text": "3.3 Optimal Segmentation",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple changepoints</span>"
    ]
  },
  {
    "objectID": "3_multiple_changes.html#introduction",
    "href": "3_multiple_changes.html#introduction",
    "title": "3  Multiple changepoints",
    "section": "",
    "text": "3.1.1 Real Example: Genomic Data and Neuroblastoma\nTo motivate this discussion, we return to the example from week 1: detecting active genomic regions using ChIP-seq data. Our goal here is to identify copy number variations (CNVs)—structural changes in the genome where DNA sections are duplicated or deleted. These variations can impact gene expression and are linked to diseases like cancer, including neuroblastoma. The dataset we’ll examine consists of logratios of genomic probe intensities, which help us detect changes in the underlying DNA structure.\nStatistically our objective is to segment this logratio sequence into regions with different means, corresponding to different genomic states:\n\n\n\n\n\n\n\n\n\nAs seen from the plot, the data is noisy, but there are visible shifts in the logratio values, suggesting multiple changes in the underlying copy number. By the end of this chapter, we will segment this sequence!\n\n\n3.1.2 Towards multiple changes\nUnder this framework, the observed sequence \\(y_t\\) can be modeled as a piecewise constant signal with changes in the mean occurring at each changepoint \\(\\tau_k\\). A plausible model for the change-in-mean signal is given by\n\\[\ny_t = \\mu_k + \\epsilon_t, \\quad \\text{for} \\ \\tau_k \\leq t &lt; \\tau_{k+1}, \\ k = 0, 1, \\dots, K,\n\\]\nwhere \\(\\mu_k\\) is the mean of the \\(k\\)-th segment, and \\(\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)\\) are independent Gaussian noise terms with mean 0 and (known) variance \\(\\sigma^2\\).\nAs a starting example, we can generate a sequence with 4 segments, with \\(\\tau_1 = 50, \\tau_2 = 100, \\tau_3 = 150\\) and means \\(\\mu_1 = 2, \\mu_2 = 0, \\mu_3 = -1\\) and \\(\\mu_4 = 2\\). Running the CUSUM statistic in this scenario with multiple changes, leads to the following \\(C_\\tau^2\\) trace:\n\n\n\n\n\n\n\n\n\nFrom this, we notice that our test still has power to detect some of the changes, but the estimate that we get, is initially wrong. \\(\\Delta \\mu = |\\mu_1 - \\mu_2|\\). Is power lost when there is more then one change in our test?\nWell, to answer this question, we can compare the values of the CUSUM statistic ran on the whole dataset (as above), with the values of the CUSUM, ran on a subset containing only one change:\n\n\nWarning: Removed 199 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\n\nWe can see that max of the old cusum (the line in grey) is much lower than the one where we isolate the sequence on one single change! So there is an effective loss of power in this scenario in analyzing all changes together, as some changes are masking the effects of others…\nThis gives us motivation to move towards some methodology that tries to estimate all changes locations jointly, rather then one at a time!\n\n\n3.1.3 The cost of a segmentation\nWell, so far we only worked with one scheme that tried to split a sequence in a hald\nBut how can we work in case we have more than one change? Well, we need to introduce the cost of a segment.\nIf we assume the data is independent and identically distributed within each segment, for segment parameter \\(\\theta\\), then this cost can be obtained through: \\[\n    \\mathcal{L}(y_{s+1:t}) = \\min_\\theta \\sum_{i = s + 1}^{t} - \\log(f(y_i, \\theta))\n\\] with \\(f(y, \\theta)\\) being the likelihood for data point \\(y\\) if the segment parameter is \\(\\theta\\). Now, for example, in the gaussian case this cost is given by:\n\\[\n\\mathcal{L}(y_{s:t}) = \\frac{1}{2\\sigma^2}  \\sum_{i = s}^{t} \\left ( y_i - \\bar{y}_{s:t} \\right)^2\n\\]\nThe cost for the full segmentation will be given by the sum across all segments:\n\\[\n\\sum_{k = 0}^K \\mathcal{L}(y_{\\tau_k+1:\\tau_{k+1}})\n\\]\nInterestingly, the cost of a full segmentation is closely related to the LR test. Consider, a single Gaussian change-in-mean at time \\(\\tau\\), splitting the data into two segments: \\(y_{1:\\tau}\\) and \\(y_{\\tau+1:n}\\). The cost of this segmentation is:\n\\[\n\\mathcal{L}(y_{1:\\tau}) + \\mathcal{L}(y_{\\tau+1:n}) = \\frac{1}{\\sigma^2} \\left[\\sum_{i=1}^{\\tau} (y_i - \\bar{y}_{1:\\tau})^2 + \\sum_{i=\\tau+1}^{n} (y_i - \\bar{y}_{(\\tau+1):n})^2 \\right]\n\\]\nWhich is essentially the same LR test as we saw last week, without the null component. Specifically, for one change, minimizing the segmentation cost over all possible changepoints locations \\(\\tau\\) is equivalent to maximizing the CUSUM statistic.\n\n\n3.1.4 The “best” segmentation\nWe now have a way of evaluating how “good” a segmentation is, so it’s only natural to ask the question: what would be the best one?\nWell, one way would be to, say, finding the the best set of \\(\\tau = \\tau_0, \\dots, \\tau_{K+1}\\) changepoints that minimise the cost:\n\\[\n\\min_{\\substack{K \\in \\mathbb{N}\\\\ \\tau_1, \\dots, \\tau_K}} \\sum_{k = 0}^K \\mathcal{L}(y_{\\tau_k+1:\\tau_{k+1}}).\n\\tag{3.1}\\]\nWhich one would this be? Say that for instance we range the \\(K = 1, \\dots, n\\), and at each step we find the best possible segmentation. Graphically, we would be observing the following:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWell, arguably we would like to stop at 4, which we know is the real number of segments, but the cost keep going down…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd finally:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWell, it turns out, that according to the minimization above, the optimal segmentation across all would be the one that puts each point into its own segment!\nWell, there are different solutions to this problem. The first one we will see, is a divide-and-conquer greedy approach, called Binary Segmentation, and the second one will aim a generating a different optimization to the one below that will find the optimal segmentation up to a constant to avoid over-fitting!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple changepoints</span>"
    ]
  },
  {
    "objectID": "3_multiple_changes.html#a-comparison-of-the-two-approaches",
    "href": "3_multiple_changes.html#a-comparison-of-the-two-approaches",
    "title": "3  Multiple changepoints",
    "section": "3.4 A Comparison of the two Approaches",
    "text": "3.4 A Comparison of the two Approaches\nWhen deciding which segmentation approach to use, Binary Segmentation (BS) and Optimal Partitioning (OP) each offer different strengths. The choice largely depends on the characteristics of the data and the goal of the analysis.\n\n3.4.1 Quality of the Segmentation\nGenerally, Optimal Partitioning (OP) provides the most accurate segmentation, especially when we have a well-defined model and expect precise changepoint detection. OP ensures that the solution is optimal by globally minimizing the cost function across all possible segmentations. This is ideal for datasets with clear changes, even if noise is present.\nLet’s consider a case with true changepoints at \\(\\tau = 100, 200, 300\\), and segment means \\(\\mu_{1:4} = 2, 1, -1, 1.5\\):\n\n\n\n\n\n\n\n\n\nWhile the underlying signal follows these clear shifts, noise complicates segmentation. Binary Segmentation uses a greedy process where each iteration looks for the largest changepoint. Although fast, this local search can make mistakes if the signal isn’t perfectly clear, particularly in the early stages of the algorithm. For example, running BS on this dataset introduces a mistake at \\(\\tau = 136\\), as shown in the plot below:\n\n\n\n\n\n\n\n\n\nThis error is carried in the subsequent steps, and the full binary segmentation algorithm will output an additional change at \\(\\tau = 136\\)… Optimal Partitioning (OP), on the other hand, evaluates all possible segmentations considers the overall fit across the entire sequence. It is therefore less susceptible to adding “ghost” changepoints, as rather than focusing on the largest change at each step.\nTo illustrate, we compare the segmentations generated by both approaches:\n\n\n\n\n\n\n\n\n\n\n\n3.4.2 Computational Complexity\nWell, you may ask why not using OP all the time, then? Well, in changepoint detection, in which is the most appropiate method, we often have to keep track of the computational performance too, and Binary Segmentation is faster on average. For this reason, for large datasets where approximate solutions are acceptable, it might be the best option.\nSpecifically:\n\nBinary Segmentation starts by dividing the entire sequence into two parts, iteratively applying changepoint detection to each segment. In the average case, it runs in \\(\\mathcal{O}(n \\log n)\\) because it avoids searching every possible split point. However, in the worst case (if all data points are changepoints), the complexity can degrade to \\(\\mathcal{O}(n^2)\\), as each step can require recalculating test statistics for a growing number of segments.\nOptimal Partitioning, on the other hand, solves the changepoint problem by recursively considering every possible split point up to time \\(t\\). The result is an optimal segmentation, but at the cost of \\(\\mathcal{O}(n^2)\\) computations. This holds true for both the average and worst cases, as it always requires a full exploration of all potential changepoints. However, as we will see in the tutorial and lab, there are recent developments that allow for a faster computation of OP, achieving average cases of \\(\\mathcal{O}(n \\log n)\\): those are the PELT and FPOP algorithms.\n\n\n\n3.4.3 Neuroblastoma example\nGood news is, on the neuroblastoma dataset, the two methods produce the same results!",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple changepoints</span>"
    ]
  },
  {
    "objectID": "1_intro_cusum.html#exercises",
    "href": "1_intro_cusum.html#exercises",
    "title": "1  An Introduction to Changepoint Detection",
    "section": "1.4 Exercises",
    "text": "1.4 Exercises\n\n1.4.1 Workshop 1\n\nDetermine if the following processes are stationary, piecewise stationary, or non-stationary:\n\n\n\n\\(y_t = y_{t - 1} + \\epsilon_t, \\quad \\ t = 2, \\dots, n, y_1 = 0, \\epsilon_{1:n} \\sim N(0, 1)\\). Suggestion: try to compute the variance of the process.\n$ y_t = t + 3* (t &gt; 50),  t = 1, , 100, N(0, 1) $\n\\(y_t = 0.05 \\cdot t + \\epsilon_t, \\ t = 1, \\dots, 100, \\quad \\epsilon_{1:100} \\sim N(0, 1)\\)\n\n\n\nIn this exercise we will show that the difference \\(\\bar{y}_{1:\\tau} - \\bar{y}_{(\\tau+1):n}\\), multiplied by the normalizing constant \\(1/\\sigma^2\\sqrt{\\frac{\\tau(n-\\tau)}{n}}\\), follows a standard normal distribution. Hint:\n\nCompute the expected value and variance of the difference\nConclude that if you standardise the sum, this follows a normal distribution.\n\n\n\n\n1.4.2 Lab 1\n\nCode the CUSUM algorithm for a unknown change location, based on the pseudocode above.\nModify your function above to output the CUSUM statistics over all ranges of tau, and recreate the Simpsons plot above.\n\n\n\n\n\nBown, Jonathan. 2023. “Simpsons Episodes & Ratings (1989-Present).” https://www.kaggle.com/datasets/jonbown/simpsons-episodes-2016?resource=download.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>An Introduction to Changepoint Detection</span>"
    ]
  },
  {
    "objectID": "3_multiple_changes.html#optimal-partitioning",
    "href": "3_multiple_changes.html#optimal-partitioning",
    "title": "3  Multiple changepoints",
    "section": "3.3 Optimal Partitioning",
    "text": "3.3 Optimal Partitioning\nAnother solution to avoid the over-fitting problem of Equation 3.1 lies in introducing a penalty term that discourages too many changepoints, avoiding overfitting. This is known as the penalised approach.\nTo achieve this, we want to minimize the following cost function:\n\\[\nQ_{n, \\beta} = \\min_{K \\in \\mathbb{N}} \\left[ \\min_{\\substack{\\\\ \\tau_1, \\dots, \\tau_K}} \\sum_{k = 0}^K \\mathcal{L}(y_{\\tau_k+1:\\tau_{k+1}}) + \\beta K\n\\right],\n\\]\nwhere \\(Q_{n, \\beta}\\) represents the optimal cost for segmenting the data up to time \\(n\\) with a penalty $ $ that increases with each additional changepoint \\(K\\). With the \\(\\beta\\) term, for every new changepoint added, the cost of the full segmentation increases, discouraging therefore models with too many changepoints.\nUnlike Binary Segmentation, which works iteratively and makes local decisions about potential changepoints, and as we have seen it is prone to errors, solving \\(Q_{n, \\beta}\\) ensures that the segmentation is globally optimal, as in the location of the changes are the best possible to minimise our cost.\nNow, directly solving this problem using a brute-force search is computationally prohibitive, as it would require checking every possible combination of changepoints across the sequence: the number of possible segmentations grows exponentially as \\(n\\) increases…\nFortunately, this problem can be solved efficiently using a sequential, dynamic programming algorithm: Optimal Partitioning (OP), from Jackson et al. (2005). OP solves ?eq-pen-cost exactly through the following recursion.\nWe start with \\(\\mathcal{Q}_{0, \\beta} = -\\beta\\), and then, for each \\(t = 1, \\dots, n\\), we compute:\n\\[\n    \\mathcal{Q}_{t, \\beta} = \\min_{0 \\leq \\tau &lt; t} \\left[ \\mathcal{Q}_{\\tau, \\beta} + \\mathcal{L}(y_{\\tau + 1:t}) + \\beta \\right].\n\\tag{3.3}\\]\nHere, \\(\\mathcal{Q}_{t, \\beta}\\) represents the optimal cost of segmenting the data up to time \\(t\\). The algorithm builds this solution sequentially by considering each possible segmentation \\(\\mathcal{Q}_{0, \\beta},\\ \\cdots, \\mathcal{Q}_{t-2, \\beta},\\ \\mathcal{Q}_{t-1, \\beta}\\) before the current time \\(t\\), plus the segment cost up to current time \\(t\\), \\(\\mathcal{L}(y_{\\tau + 1:t})\\).\n\n3.3.1 Optimal partitinioning in action\nThis recursion can be quite hard to digest, and is, as usual, best described graphically.\nStep 1 Say we are at \\(t = 1\\). In this case, according to equation above, the optimal cost up to time one will be given by (remember that the \\(\\beta\\) cancels out with \\(Q_{0, \\beta}\\)!):\n\\[\n\\mathcal{Q}_{1, \\beta} = \\left[ -\\beta + \\mathcal{L}(y_{1:1}) + \\beta \\right] = \\mathcal{L}(y_{1:1})\n\\]\n\n\n\n\n\n\n\n\n\nStep 2. Now, at the second step, we have to minimise between two segmentations:\n\nOne with the whole sequence in a second segment alone (again, \\(\\beta\\) cancels out with \\(Q_{0, \\beta} = -\\beta\\)), and this will be given by \\(\\mathcal{L}(y_{1:2})\\) (dotted line)\nOne with the optimal segmentation from step 1 \\(\\mathcal{Q}_{1, \\beta}\\) (whose cost considered only the first point in its own segment!), to which we have to sum the cost relative to a second segment \\(\\mathcal{L}(y_{2:2})\\) that puts the second point alone, and the penalty \\(\\beta\\) as we have added a new segment!\n\nWe minimise across the two, and this gives us \\(Q_{2, \\beta}\\).\n\n\n\n\n\n\n\n\n\nStep 3: Similarly, at \\(t = 3\\) we have now three segmentations to choose from:\n\nThe one that puts the first three observations in the same segment, whose cost will be given simply by \\(\\mathcal{L}(y_{1:2})\\),\nThe one considering the optimal segmentation from time 1, plus the cost of adding an extra segment with observation 2 and 3 together\nFinally the optimal from segmentation 2, \\(\\mathcal{Q}_{2, \\beta}\\), plus the segment cost of fitting an extra segment with point 3 alone. Note that \\(\\mathcal{Q}_{2, \\beta}\\) will come from the step before: if we would have been beneficial to add a change, at the previous step, this information is carried over!\n\nAgain, we pick the minimum across these three to get \\(\\mathcal{Q}_{3, \\beta}\\), and proceed.\n\n\n\n\n\n\n\n\n\nStep \\(n\\) Until the last step! Which would look something like this:\n\n\n\n\n\n\n\n\n\nA formal description of the algorithm can be found below:\n\nINPUT: Time series \\(y = (y_1, ..., y_n)\\), penalty \\(\\beta\\)\nOUTPUT: Optimal changepoint vector \\(cp_n\\)\n\nInitialize \\(\\mathcal{Q}_0 \\leftarrow -\\beta\\)\nInitialize \\(cp_0 \\leftarrow \\{\\}\\)\n\nFOR \\(t = 1, \\dots, n\\)\n     \\(\\mathcal{Q}_t \\leftarrow \\min_{0 \\leq \\tau &lt; t} \\left[ \\mathcal{Q}_{\\tau} + \\mathcal{L}(y_{\\tau + 1:t}) + \\beta \\right]\\)\n     \\(\\hat\\tau \\leftarrow \\text{arg}\\min_{0 \\leq \\tau &lt; t} \\left[ \\mathcal{Q}_{\\tau} + \\mathcal{L}(y_{\\tau + 1:t}) + \\beta \\right]\\)\n     \\(cp_t \\leftarrow (cp_\\hat\\tau, \\hat\\tau)\\) // Append the changepoint to the list at the last optimal point\n\nRETURN \\(cp_n\\)\n\nRunning the Optimal Partitioning method on our example scenario, with the same penalty \\(\\beta = 2 \\log(400) =\\) 11.98 as above, gives changepoint locations \\(\\tau_{1:4} = \\{100, 203, 301\\}\\).\n\n\nLoading required package: zoo\n\n\n\nAttaching package: 'zoo'\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\nSuccessfully loaded changepoint package version 2.2.4\n See NEWS for details of changes.\n\n\n\n\n\n\n\n\n\nSo we can see how on this dataset in particular, OP performs slightly better then Binary Segmentation on the last change, getting closer to the real changepoint of 300!\n\n\n3.3.2 Neuroblastoma example\nRunning the two methods on the neuroblastoma example below, gives exactly the same two segmentations:",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple changepoints</span>"
    ]
  },
  {
    "objectID": "2_control.html#exercises",
    "href": "2_control.html#exercises",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\n\n2.4.1 Workshop 2\n\nGiven the asymptotic result on the distribution, for \\(n = 100\\) and a false positive rate \\(\\alpha = 0.05\\), calculate the threshold \\(c\\).\nShow that the likelihood ratio test statistic for a change in mean in a Gaussian model is the CUSUM statistics squared, rescaled by the known variance, e.g. \\(LR_\\tau = C_\\tau^2/\\sigma^2\\).\n\n\n\n2.4.2 Lab 2\n\nWrite a function, that taking as input \\(n\\) and a desired \\(\\alpha\\) level for false positive rate, returns the treshold for the cusum statistics.\nConstruct a function that, taking as input \\(n\\), a desired \\(\\alpha\\) , and a replicates parameter, runs a Monte Carlo simulation to tune an empirical penalty for the CUSUM change-in-mean on a simple Gaussian signal. Tip: You can reuse the function for computing the CUSUM statistics that you built the last week\nCompare for a range of increasingly values of n, e.g. \\(n = 100, 500, 1000, 10.000\\), and for few desired levels of alpha, the Monte Carlo threshold with the theoretically justified threshold. Plot the results, to recreate the plot above.\nUsing the Test the Simpsons dataset, find a critical level for your CUSUM statistics, and declare a change with the change-in-mean model.\n\n\n\n\n\nBaranowski, Rafal, Yining Chen, and Piotr Fryzlewicz. 2019. “Narrowest-over-Threshold Detection of Multiple Change Points and Change-Point-Like Features.” Journal of the Royal Statistical Society Series B: Statistical Methodology 81 (3): 649–72.\n\n\nYao, Yi-Ching, and Richard A Davis. 1986. “The Asymptotic Behavior of the Likelihood Ratio Statistic for Testing a Shift in Mean in a Sequence of Independent Normal Variates.” Sankhyā: The Indian Journal of Statistics, Series A, 339–53.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#workshop-2",
    "href": "2_control.html#workshop-2",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.5 Workshop 2",
    "text": "2.5 Workshop 2\n\nGiven the asymptotic result on the distribution, for \\(n = 100\\) and a false positive rate \\(\\alpha = 0.05\\), calculate the threshold \\(c\\).\nShow that the likelihood ratio test statistic for a change in mean in a Gaussian model is the CUSUM statistics squared, rescaled by the known variance, e.g. \\(LR_\\tau = C_\\tau^2/\\sigma^2\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "2_control.html#lab-2",
    "href": "2_control.html#lab-2",
    "title": "2  Controlling the CUSUM and Other Models",
    "section": "2.6 Lab 2",
    "text": "2.6 Lab 2\n\nWrite a function, that taking as input \\(n\\) and a desired \\(\\alpha\\) level for false positive rate, returns the treshold for the cusum statistics.\nConstruct a function that, taking as input \\(n\\), a desired \\(\\alpha\\) , and a replicates parameter, runs a Monte Carlo simulation to tune an empirical penalty for the CUSUM change-in-mean on a simple Gaussian signal. Tip: You can reuse the function for computing the CUSUM statistics that you built the last week\nCompare for a range of increasingly values of n, e.g. \\(n = 100, 500, 1000, 10.000\\), and for few desired levels of alpha, the Monte Carlo threshold with the theoretically justified threshold. Plot the results, to recreate the plot above.\nUsing the Test the Simpsons dataset, find a critical level for your CUSUM statistics, and declare a change with the change-in-mean model.\n\n\n\n\n\nBaranowski, Rafal, Yining Chen, and Piotr Fryzlewicz. 2019. “Narrowest-over-Threshold Detection of Multiple Change Points and Change-Point-Like Features.” Journal of the Royal Statistical Society Series B: Statistical Methodology 81 (3): 649–72.\n\n\nYao, Yi-Ching, and Richard A Davis. 1986. “The Asymptotic Behavior of the Likelihood Ratio Statistic for Testing a Shift in Mean in a Sequence of Independent Normal Variates.” Sankhyā: The Indian Journal of Statistics, Series A, 339–53.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Controlling the CUSUM and Other Models</span>"
    ]
  },
  {
    "objectID": "3_multiple_changes.html#exercise",
    "href": "3_multiple_changes.html#exercise",
    "title": "3  Multiple changepoints",
    "section": "3.5 Exercise",
    "text": "3.5 Exercise\n\n3.5.1 Workshop 3\n\nIn OP, we can reduce the numbers of checks to be performed at each iteration, reducing the complexity. This operation is called pruning. Specifically, on the condition that there exists a constant \\(\\kappa\\) such that for every \\(l &lt; t &lt; u\\): \\[\n    \\mathcal{L}(y_{l + 1:t}) + \\mathcal{L}(y_{t + 1:u}) + \\kappa \\leq \\mathcal{L}(y_{l + 1:u})\n\\] It is possible to prune without resorting to an approximation. This is the computational trick employed by the PELT algorithm, introduced in (Killick?). The PELT algorithm – acronym for Pruned Exact Linear Time – solves exactly the penalised minimization of \\(\\ref{eq:penalised-approach-mimimization}\\) with an expected computational cost that can be linear in \\(n\\) – while still retaining \\(\\mathcal{O}(n^2)\\) computational complexity in the worst case. This is achieved by reducing the number of segment costs to evaluate at each iteration via an additional pruning step based on Condition Equation 3.3. That is, if \\[\\mathcal{Q}\\tau + \\mathcal{L}(y{\\tau + 1:t}) + \\kappa \\geq \\mathcal{Q}_t\\] then we can safely prune the segment cost related to \\(\\tau\\), as \\(\\tau\\) will never be the optimal changepoint location up to any time \\(T &gt; t\\) in the future. On the basis of this information, your task is to change the Optimal Partitioning algorithm, adding an additional step to reflect the PELT pruning.\n\n\n\n3.5.2 Lab 3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple changepoints</span>"
    ]
  },
  {
    "objectID": "3_multiple_changes.html#exercises",
    "href": "3_multiple_changes.html#exercises",
    "title": "3  Multiple changepoints",
    "section": "3.4 Exercises",
    "text": "3.4 Exercises\n\n3.4.1 Workshop 3\n\n\n3.4.2 Lab 3\n\nCode the Optimal Partitioning algorithm for the Gaussian change-in-mean case.\nTips:\n\nYou can pre-compute all the possible \\(\\mathcal{L(y_{l:u})}\\), for \\(u \\geq l\\) to save computational time, and avoiding recomputing all costs.\nBe very careful with indexing… R starts indexing at 1, however, in the pseudocode, you have one element that starts at 0…\n\nInstall the changepoint package. The example signal of Section 3.4, with the same penalty value of \\(\\beta = 2 * log(n)\\), compare your implementation of OP, Binary Segmentation, and PELT. You will learn how to do so in the documentation, running ?cpt_mean. You should make 300 replicates each of your experiment, and compare:\n\nThe absolute error loss from the mean signal, e.g. \\(||\\mu_{1:n} - \\hat{\\mu}_{1:n}||^2_2\\)\nThe absolute error in the number of changepoints reconstructed, e.g. \\(|K - \\hat{K}|\\). What do we observe?\nThe runtime for one sequence of PELT agains Binary Segmentation. Evaluate the runtime with package microbenchmark.\n\n\n\n\n\n\nJackson, Brad, Jeffrey Scargle, D. Barnes, S. Arabhi, A. Alt, P. Gioumousis, E. Gwin, P. Sangtrakulcharoen, L. Tan, and Tun Tsai. 2005. “An Algorithm for Optimal Partitioning of Data on an Interval.” Signal Processing Letters, IEEE 12 (March): 105–8. https://doi.org/10.1109/LSP.2001.838216.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multiple changepoints</span>"
    ]
  },
  {
    "objectID": "4_algos_and_penalties.html",
    "href": "4_algos_and_penalties.html",
    "title": "4  PELT, WBS and Penalty choices",
    "section": "",
    "text": "4.1 Drawbacks of OP and BS\nWhen deciding which segmentation approach to use, Binary Segmentation (BS) and Optimal Partitioning (OP) each offer different strengths. The choice largely depends on the characteristics of the data and the goal of the analysis.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PELT, WBS and Penalty choices</span>"
    ]
  },
  {
    "objectID": "4_algos_and_penalties.html#a-comparison-of-the-two-approaches",
    "href": "4_algos_and_penalties.html#a-comparison-of-the-two-approaches",
    "title": "4  PELT, WBS and Penalty choices",
    "section": "",
    "text": "4.1.1 Quality of the Segmentation\nGenerally, Optimal Partitioning (OP) provides the most accurate segmentation, especially when we have a well-defined model and expect precise changepoint detection. OP ensures that the solution is optimal by globally minimizing the cost function across all possible segmentations. This is ideal for datasets with clear changes, even if noise is present.\nLet’s consider a case with true changepoints at \\(\\tau = 100, 200, 300\\), and segment means \\(\\mu_{1:4} = 2, 1, -1, 1.5\\):\n\n\n\n\n\n\n\n\n\nWhile the underlying signal follows these clear shifts, noise complicates segmentation. Binary Segmentation uses a greedy process where each iteration looks for the largest changepoint. Although fast, this local search can make mistakes if the signal isn’t perfectly clear, particularly in the early stages of the algorithm. For example, running BS on this dataset introduces a mistake at \\(\\tau = 136\\), as shown in the plot below:\n\n\n\n\n\n\n\n\n\nThis error is carried in the subsequent steps, and the full binary segmentation algorithm will output an additional change at \\(\\tau = 136\\)… Optimal Partitioning (OP), on the other hand, evaluates all possible segmentations considers the overall fit across the entire sequence. It is therefore less susceptible to adding “ghost” changepoints, as rather than focusing on the largest change at each step.\nTo illustrate, we compare the segmentations generated by both approaches:\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Computational Complexity\nWell, you may ask why not using OP all the time, then? Well, in changepoint detection, in which is the most appropiate method, we often have to keep track of the computational performance too, and Binary Segmentation is faster on average. For this reason, for large datasets where approximate solutions are acceptable, it might be the best option.\nSpecifically:\n\nBinary Segmentation starts by dividing the entire sequence into two parts, iteratively applying changepoint detection to each segment. In the average case, it runs in \\(\\mathcal{O}(n \\log n)\\) because it avoids searching every possible split point. However, in the worst case (if all data points are changepoints), the complexity can degrade to \\(\\mathcal{O}(n^2)\\), as each step can require recalculating test statistics for a growing number of segments.\nOptimal Partitioning, on the other hand, solves the changepoint problem by recursively considering every possible split point up to time \\(t\\). The result is an optimal segmentation, but at the cost of \\(\\mathcal{O}(n^2)\\) computations. This holds true for both the average and worst cases, as it always requires a full exploration of all potential changepoints. However, as we will see in the tutorial and lab, there are recent developments that allow for a faster computation of OP, achieving average cases of \\(\\mathcal{O}(n \\log n)\\): those are the PELT and FPOP algorithms.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PELT, WBS and Penalty choices</span>"
    ]
  },
  {
    "objectID": "4_algos_and_penalties.html#drawbacks-of-op-and-bs",
    "href": "4_algos_and_penalties.html#drawbacks-of-op-and-bs",
    "title": "4  PELT, WBS and Penalty choices",
    "section": "",
    "text": "4.1.1 Quality of the Segmentation\nGenerally, Optimal Partitioning (OP) provides the most accurate segmentation, especially when we have a well-defined model and expect precise changepoint detection. OP ensures that the solution is optimal by globally minimizing the cost function across all possible segmentations. This is ideal for datasets with clear changes, even if noise is present.\nLet’s consider a case with true changepoints at \\(\\tau = 100, 200, 300\\), and segment means \\(\\mu_{1:4} = 2, 1, -1, 1.5\\):\n\n\n\n\n\n\n\n\n\nWhile the underlying signal follows these clear shifts, noise complicates segmentation. Binary Segmentation uses a greedy process where each iteration looks for the largest changepoint. Although fast, this local search can make mistakes if the signal isn’t perfectly clear, particularly in the early stages of the algorithm. For example, running BS on this dataset introduces a mistake at \\(\\tau = 136\\), as shown in the plot below:\n\n\n\n\n\n\n\n\n\nThis error is carried in the subsequent steps, and the full binary segmentation algorithm will output an additional change at \\(\\tau = 136\\)… Optimal Partitioning (OP), on the other hand, evaluates all possible segmentations considers the overall fit across the entire sequence. It is therefore less susceptible to adding “ghost” changepoints, as rather than focusing on the largest change at each step.\nTo illustrate, we compare the segmentations generated by both approaches:\n\n\n\n\n\n\n\n\n\n\n\n4.1.2 Computational Complexity\nWell, you may ask why not using OP all the time, then? Well, in changepoint detection, in which is the most appropiate method, we often have to keep track of the computational performance too, and Binary Segmentation is faster on average. For this reason, for large datasets where approximate solutions are acceptable, it might be the best option.\nSpecifically:\n\nBinary Segmentation starts by dividing the entire sequence into two parts, iteratively applying changepoint detection to each segment. In the average case, it runs in \\(\\mathcal{O}(n \\log n)\\) because it avoids searching every possible split point. However, in the worst case (if all data points are changepoints), the complexity can degrade to \\(\\mathcal{O}(n^2)\\), as each step can require recalculating test statistics for a growing number of segments.\nOptimal Partitioning, on the other hand, solves the changepoint problem by recursively considering every possible split point up to time \\(t\\). The result is an optimal segmentation, but at the cost of \\(\\mathcal{O}(n^2)\\) computations. This holds true for both the average and worst cases, as it always requires a full exploration of all potential changepoints.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PELT, WBS and Penalty choices</span>"
    ]
  },
  {
    "objectID": "4_algos_and_penalties.html#pelt-and-wbs",
    "href": "4_algos_and_penalties.html#pelt-and-wbs",
    "title": "4  PELT, WBS and Penalty choices",
    "section": "4.2 PELT and WBS",
    "text": "4.2 PELT and WBS\nGood news is, despite both algorithms have drawbacks, following recent developments, those have been solved. In the next sections, we will introduce two new algorithms, PELT and WBS.\n\n4.2.1 PELT: an efficient solution to OP\nIn OP, we can reduce the numbers of checks to be performed at each iteration, reducing the complexity. This operation is called pruning. Specifically, on the condition that there exists a constant \\(\\kappa\\) such that for every \\(l &lt; t &lt; u\\):\n\\[\n        \\mathcal{L}(y_{l + 1:t}) + \\mathcal{L}(y_{t + 1:u}) + \\kappa \\leq \\mathcal{L}(y_{l + 1:u})\n\\]\nIt is possible to prune without resorting to an approximation. For many cost functions, such as the Gaussian cost, such a constant exists. Equating \\(\\kappa\\) to the penalty \\(\\beta\\), gives us a computational trick to improve on the efficiency… The PELT algorithm – acronym for Pruned Exact Linear Time – (Killick, Fearnhead, and Eckley (2012)) solves exactly the penalised minimization of Equation 3.3 with an expected computational cost that can be linear in \\(n\\) – while still retaining \\(\\mathcal{O}(n^2)\\) computational complexity in the worst case. This is achieved by reducing the number of segment costs to evaluate at each iteration via an additional pruning step based on Condition Equation 3.3. That is, if \\[\\mathcal{Q}\\tau + \\mathcal{L}(y_{\\tau + 1:t}) + \\kappa \\geq \\mathcal{Q}_t\\] then we can safely prune the segment cost related to \\(\\tau\\), as \\(\\tau\\) will never be the optimal changepoint location up to any time \\(T &gt; t\\) in the future. On the basis of this information, your task is to write the PELT algorithm. You can do this by changing the Optimal Partitioning algorithm, adding an additional step to reflect the PELT pruning.\n\n\n4.2.2 WBS: Improving on Binary Segmentation\nIn BS, one of the issues that may arise, is an incorrect segmentation. WBS, Fryzlewicz (2014), is a multiple changepoints procedures that improve on the BS changepoint estimation via computing the initial segmentation cost of BS multiple times over \\(M + 1\\) random subsets of the sequence, \\(y_{s_1:t_1}, \\dots, y_{s_M:t_M}, y_{1:n}\\), picking the best subset according to what achieves the smallest segmentation cost and reiterating the procedure over that sample accordingly. The idea behind WBS lies in the fact that a favourable subset of the data \\(y_{s_m:t_m}\\) could be drawn which contains a true change sufficiently separated from both sides \\(s_m, t_m\\) of the sequence. By the inclusion of the \\(y_{1:n}\\) entire sequence amongst the subsets, it is guaranteed that WBS will do no worse than the simple BS algorithm.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PELT, WBS and Penalty choices</span>"
    ]
  },
  {
    "objectID": "4_algos_and_penalties.html#penalty-selection",
    "href": "4_algos_and_penalties.html#penalty-selection",
    "title": "4  PELT, WBS and Penalty choices",
    "section": "4.3 Penalty selection",
    "text": "4.3 Penalty selection\n\n4.3.1 Mean absolute deviation\n\n\n4.3.2 CROPS\n\n\n\n\nFryzlewicz, Piotr. 2014. “Wild binary segmentation for multiple change-point detection.” Annals of Statistics 42: 2243–81.\n\n\nKillick, R., P. Fearnhead, and I. A. Eckley. 2012. “Optimal Detection of Changepoints with a Linear Computational Cost.” Journal of the American Statistical Association 107 (500): 1590–98.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>PELT, WBS and Penalty choices</span>"
    ]
  }
]